{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will experiment with a toy regression problem, in which we generate synthetic data (from a known p(x,y)), creating data sets of varying sizes. For each size $N$, we repeat $m$ times the following experiment: \n",
    "\n",
    "* draw a data set $(X,y)$ of size $N$\n",
    "* fit a model (with a given degree $d$ of polynomial fit, and $\\lambda$ parameter of ridge regression\n",
    "\n",
    "Once we are done, we can plot the $m$ models as well as their average; this gives us a visual illustration of the behavior of a particular model family (defined by $d$ and $\\lambda$) on data of given size.\n",
    "\n",
    "We can also analyze the different components that contribute to the (expected, test-time) loss of these models. That's the main contribution you will have to make to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.max_open_warning': 0}) # to supress an annoying warning about too many figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions for out data generator and related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utility functions provided by us\n",
    "\n",
    "def degexpand(X, deg, C=None):\n",
    "    \"\"\"\n",
    "    Prepares data matrix with a column of ones and polynomials of specified\n",
    "    degree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        X : 2D array\n",
    "            n x d data matrix (row per example)\n",
    "        deg : integer\n",
    "            Degree of polynomial\n",
    "        C : 1D array\n",
    "            Scaling weights. If not specifed, weights are calculated to fit each\n",
    "            columns of X in [-1, 1].\n",
    "            Note: It is shown in problem set 1 that this normalization does\n",
    "            not affect linear regression, as long as it is applied\n",
    "            consistently to training *and* test data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        out_X : 2D array\n",
    "            n x (2 * d + 1) data matrix (row per example)\n",
    "            The output is arranged as follows:\n",
    "            <see solutions for PS1>\n",
    "        C : 1D array\n",
    "            Scaling weights that were used. Can be used in scaling other data later in the same way.\n",
    "    \"\"\"\n",
    "    assert X.ndim == 2\n",
    "    n, m = X.shape\n",
    "    # Make polynomials\n",
    "    out_X = (X[..., np.newaxis] ** (1. + np.arange(deg))).reshape(n, -1)\n",
    "\n",
    "    # Add column of ones\n",
    "    out_X = np.concatenate([np.ones((out_X.shape[0], 1)), out_X], axis=1)\n",
    "\n",
    "    if C is None:\n",
    "        C = abs(out_X).max(axis=0)\n",
    "    else:\n",
    "        assert np.shape(C) == (out_X.shape[1],), \"C must match outgoing matrix\"\n",
    "\n",
    "    out_X /= C # divide dimension-wise\n",
    "    return out_X, C\n",
    "\n",
    "\n",
    "def gen_samples(fn, domain=(-5,5),num_samples=3, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Main building block for the data generator\n",
    "    Produce num_samples from p(x,y) where\n",
    "    x ~ uniform(domain), y|x~fn(x)+nu,  nu~N(0,sigma)\n",
    "    \n",
    "    Returns 2D array of shape (num_samples,2)\n",
    "    \"\"\"\n",
    "    low, high = domain\n",
    "\n",
    "    # sample points\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        _input = random.uniform(low, high)\n",
    "        data.append( [_input, fn(_input)] )\n",
    "    data = np.array(data)\n",
    "\n",
    "    # sample some guassian noise\n",
    "    noise = np.random.normal(loc=0.0, scale=sigma, size=num_samples)\n",
    "    \n",
    "    data[:,1] += noise\n",
    "    \n",
    "    return data\n",
    "\n",
    "def gen_train_data(fn, num_samples=3, num_sets=30, domain=(-5,5), sigma=1.0):\n",
    "    \"\"\"\n",
    "    Generate num_sets datasets, each with num_samples points, drawn i.i.d. from\n",
    "    p(x,y) defined by domain, fn, sigma (see comment for gen_samples)\n",
    "    \n",
    "    Returns data of shape (num_sets,num_samples,2) where (:,:,0) are xs and (:,:,1) are ys\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.array([gen_samples(\n",
    "        fn, num_samples=num_samples, domain=domain, sigma=sigma) for _ in range(num_sets)])\n",
    "    return data\n",
    "\n",
    "def gen_val_data(fn, num_samples=100, num_sets=30,domain=(-5,5), sigma=1.0):\n",
    "    \"\"\"\n",
    "    Generate validation data with num_samples drawn i.i.d. from p(x,y)\n",
    "    where x~uniform(domain), y|x ~ fn(x)+nu, nu~N(0,sigma)\n",
    "    \n",
    "    This generates a set of sets, similar to gen_train_data, but for val, we get the same X\n",
    "    in all the sets, but different ys (varying according to the non-deterministic model component)\n",
    "    Since we want this property, the code is a bit different from gen_train_data\n",
    "    \"\"\"\n",
    "    \n",
    "    low, high = domain\n",
    "\n",
    "    data=np.zeros([num_sets,num_samples,2])\n",
    "    \n",
    "    np.random.seed(789123) # fix random number generator seed for consistency; e\n",
    "                           # very time you run this you should get same val set\n",
    "                        \n",
    "    \n",
    "    # sample values of x (once)\n",
    "    xs = np.random.uniform(low,high,num_samples)\n",
    "    Fx = fn(xs) # the deterministic portion of y (shared by all num_sets val sets)\n",
    "    \n",
    "    for s in range(num_sets):\n",
    "        # add random noise (different for each instance in each set)\n",
    "        noise = np.random.normal(loc=0.0, scale=sigma, size=num_samples)\n",
    "        data[s,:,:]=np.stack((xs,Fx+noise),axis=1)\n",
    "    \n",
    "    # rerandomize. Comment out for deterministic run\n",
    "    from datetime import datetime\n",
    "    np.random.seed(int(round(datetime.now().timestamp())))\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the problem using closed form solution to ridge regreesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form_reg_solution(train, lamb, degree): \n",
    "    \"\"\"\n",
    "    Closed form solution for ridge regression, for a givem regularization parameter 'lamb' and polynomial\n",
    "    model degree 'degree'\n",
    "    Expects a set of training sets, and returns a corresponding set of models\n",
    "    train is of shape (M,N,2) where train(m,:,0) and train(m,:,1) are xs and ys of the m-th set\n",
    "\n",
    "    Fit (train) a model for each set, and return a list of models. Each model is a dictionary:\n",
    "    'w' -> the model parameters w\n",
    "    'C' -> the feature scaling coefficients (applied to phi(x) before fitting or deploying the model)\n",
    "    'deg' -> the polynomial degree (complexity) of the model\n",
    "    'lambda' -> the regularization parameter for ridge regression\n",
    "    \"\"\"\n",
    "    \n",
    "    models=[]\n",
    "    \n",
    "    for train_data in train:\n",
    "        # extract x,y from train_data\n",
    "        x, y = train_data[:,:-1], train_data[:,1].reshape(-1,1)\n",
    "    \n",
    "        # map x to (normalized) feature maps\n",
    "        x, C = degexpand(x, degree)\n",
    " \n",
    "        N, m = x.shape\n",
    "\n",
    "        I = np.eye(m)\n",
    "        tmp = np.dot(x.T, x)\n",
    "        tmp += lamb * I\n",
    "        \n",
    "        w = np.linalg.pinv(tmp).dot(x.T).dot(y)\n",
    "        # numerically stable alternative courtesy of https://stackoverflow.com/a/54772323/6416660\n",
    "        # w = np.linalg.solve(tmp, x.T.dot(y))\n",
    "\n",
    "        # record the model\n",
    "        models.append({'w': w, 'C': C, 'deg': degree, 'lambda': lamb})\n",
    "        \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, models, domain=(-5,5),ax=None):\n",
    "    \"\"\"\n",
    "    Plot the data set (1D x->y) as blue dots, and the function predicted by each model in the list 'models' \n",
    "    as a red line (all the models are used, yielding a \"bundle\" of lines)\n",
    "    The average prediction (averaged over the models for every x) is plotted as a cyan line\n",
    "    \"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax=plt.axes()\n",
    "    \n",
    "    # sample points along x to interpolate\n",
    "    x_range = np.linspace(domain[0], domain[-1], 100)\n",
    "    x_range = x_range.reshape(-1,1)\n",
    "    mean_y = np.zeros(x_range.shape)\n",
    "    \n",
    "    for m in models:\n",
    "        x_values, _ = degexpand(x_range, m['deg'], m['C'])\n",
    "        yhat=np.dot(x_values,m['w'])\n",
    "        mean_y = mean_y+yhat/len(models)\n",
    "        ax.plot(x_range, yhat, 'r-', linewidth = 1,alpha=.3)\n",
    "    if data is not None:\n",
    "        ax.plot(data[:,0], data[:,1], 'b.',markersize=2)\n",
    "    ax.plot(x_range,mean_y,'-',c='xkcd:cyan',linewidth=3)\n",
    "    ax.grid(True)\n",
    "    ax.set_ylim(-13,5.5)\n",
    "    ax.set_title(\"lambda=%.2f\" % (m['lambda']))\n",
    "#     plt.savefig(name+\"samp_my_fig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function we want to model\n",
    "fn = lambda x: -0.02*x**4 + 0.1*x**3 + 0.3*x**2 - 1.2*x\n",
    "# this is an example of Python's \"lambda expression\" which sort of defines an unnamed function. \n",
    "# Try playing with the coefficients, or maybe add terms, to get differently shaped functions\n",
    "    \n",
    "# the domain of the function we're sampling from\n",
    "domain=(-5,5)\n",
    "\n",
    "# std of the guassian noise to be added\n",
    "sigma = 0.5\n",
    "\n",
    "# we will explore these regularization parameters\n",
    "lambdas = [0,1.0,50.0]\n",
    "# and these Ns\n",
    "Ns = [5,10,25,50,100]\n",
    "\n",
    "# generate a val set (will be shared in all the experiments)\n",
    "# make it large to get a meaningful approximation for the test loss\n",
    "val = gen_val_data(fn, num_sets=1000, num_samples=1000, domain=domain, sigma=sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting (running the experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we will collect all the models fit in these experiments into a dictionary\n",
    "models=dict()\n",
    "\n",
    "# this is to make plots more legible\n",
    "model_names={1: \"linear\", 2:\"quadratic\", 3:\"3rd order\", 4:\"4th order\",6: \"6th order\"}\n",
    "\n",
    "for deg in [1,2,3,4,6]: # for each model type\n",
    "    for N in Ns: # sample 100 datasets for each N we consider\n",
    "        train=gen_train_data(fn, num_samples=N, num_sets=100, domain=domain, sigma=sigma)\n",
    "        # set up the figure\n",
    "        fig, ax = plt.subplots(1,len(lambdas),figsize=(15,6))\n",
    "        for l in range (len(lambdas)): \n",
    "            # now, for each lambda, fit 100 models of degree d with lambda as the regularization paramter\n",
    "            # Each model is fit to N datapoints drawn from p(x,y)\n",
    "            m_index = \"N%d, d%d, l%.2f\"%(N,deg,lambdas[l]) # string identifying the experiment\n",
    "            models[m_index]=closed_form_reg_solution(train,lambdas[l],degree=deg) \n",
    "            # visualize the models along with a subset of points from p(x,y)\n",
    "            plot(val[0,:200,:], models[m_index], domain=domain,ax=ax[l])\n",
    "        fig.suptitle('%s model N=%d'%(model_names[deg],N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis (TO DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part you will need to implement; see the Pset for details of what you are asked to do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f594f625523eedf37de17555ec746ece4289f4fa39150ea03385bfab3da2cc11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
