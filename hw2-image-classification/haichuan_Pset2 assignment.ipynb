{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Predefined ‘magic function’ that display image inline\n",
    "import matplotlib.image as mimg\n",
    "# package to help us read in images\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "rng=default_rng()  # this is a recommended way to use random number generation now\n",
    "import os\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict={0:'T-shirt/top',\n",
    "              1:'Trouser',\n",
    "              2:'Pullover',\n",
    "              3:'Dress',\n",
    "              4:'Coat',\n",
    "              5:'Sandal',\n",
    "              6:'Shirt',\n",
    "              7:'Sneaker',\n",
    "              8:'Bag',\n",
    "              9:'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataBase(basename,partition='train',N=None, shuffle=True,normalize=True):\n",
    "    '''\n",
    "    This function will create one database that will contain images with their lables\n",
    "    The data are supposed to be in the paths consisting of\n",
    "     <basename> / <partition> / <category> / <category>-<index>.jpg\n",
    "     e.g., FashionMNIST/val/4/4-37.jpg\n",
    "    Inputs:\n",
    "        basename (str)  : name of the folder containing all the data. It should be \"FashionMNIST\"\n",
    "        partition (str) : \"train\" or \"val\" or \"test\"\n",
    "        N (int)         : number of examples for each category; when N=None, all samples will be loaded\n",
    "        shuffle (bool)  : boolean value; if False, samples from category 0 will be returned first and samples from category 9 last\n",
    "                          if True, samples will be randomly shuffled\n",
    "        normalize (bool): boolean value; if True, samples are normalized to [-1,1]\n",
    "    Outputs:\n",
    "        database (list) : list of tuples (x,y). x is image data. y is numeric label of x \n",
    "        \n",
    "    '''\n",
    "    database=[]\n",
    "    for label in range(10): # note: we hardcode number of classes here, it could be another argument\n",
    "        n = len(os.listdir(os.path.join(basename,partition,str(label)))) if N is None else N\n",
    "        for i in range(n):          \n",
    "            imageName=os.path.join(basename,partition,str(label),str(label)+'-'+str(i)+'.jpg')\n",
    "            imageData=mimg.imread(imageName)\n",
    "            imageData = np.float32(imageData) if not normalize else np.float32(imageData)/255*2-1\n",
    "            database.append((imageData,label))\n",
    "    if shuffle:\n",
    "        random.shuffle(database)\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2ndarray(dataset):\n",
    "    '''\n",
    "    This function will take the output from makeDataBase() and return two numpy arrays: X and y\n",
    "    Inputs:\n",
    "        dataset (list): list of (sample, label) pairs\n",
    "    Outputs:\n",
    "        X (array)     : a 2D numpy array with size (N,D). N is the length of dataset, D is 28*28. \n",
    "                        Each row of X is an image sample flattened\n",
    "        y (array)     : a numpy array with size (N,). y contains numeric labels of corresponding samples\n",
    "    '''\n",
    "    num=len(dataset)\n",
    "    X = np.empty((num,28*28),dtype=np.float32)\n",
    "    y = np.empty((num,),dtype=int)\n",
    "    for i in range(num):\n",
    "        X[i] = dataset[i][0].flatten()\n",
    "        y[i] = dataset[i][1]\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(yhat,y):\n",
    "    '''\n",
    "    This function takes predicted labels and ground truth labels and return accuracy\n",
    "    Inputs:\n",
    "        y_hat (array): (N,)-shaped numpy array containing predicted labels OR\n",
    "                       (N,C) one-hot representation for C classes\n",
    "        y (array)    : (N,)-shaped numpy array containing ground truth labels OR\n",
    "                       (N,C) one-hot representation for C classes\n",
    "    Outputs:\n",
    "        accu (float) : accuracy between [0.,1.]\n",
    "    '''\n",
    "    # if needed, convert from one-hot to class labels\n",
    "    if len(y.shape) ==2:\n",
    "        y = np.argmax(y,axis=1)\n",
    "    if len(yhat.shape) ==2:\n",
    "        yhat=np.argmax(yhat,axis=1)\n",
    "    accu = np.count_nonzero(yhat==y)/len(y)\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to modify the path below to point to wherever you stored the data in Pset 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = makeDataBase('../ps1/FashionMNIST',partition='train',normalize=True) # this loads the entire training set, you\n",
    "                                                                                # may want to use N=200 or other N for some steps\n",
    "trainX, trainy = list2ndarray(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = makeDataBase('../ps1/FashionMNIST',partition='val',normalize=True)\n",
    "testset = makeDataBase('../ps1/FashionMNIST',partition='test',normalize=True)\n",
    "valX, valy = list2ndarray(valset)\n",
    "testX, testy = list2ndarray(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of functions implementing different schedules for the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expSchedule(lr,e,delta):\n",
    "    '''\n",
    "    This function computes the updates learning rate (lr) at epoch e, using the decay factor delta;\n",
    "    it returns the new lr which is the old lr multiplied by delta (e is ignored)\n",
    "    '''\n",
    "    return lr*delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepSchedule(lr,t,steps):\n",
    "    '''\n",
    "    A scheduled adjustment of the learning rate.\n",
    "    The current learning rate at the end of epoch e is updated according to the argument 'steps':\n",
    "    steps specifies the schedule; it's a list, with i-th element being a tuple\n",
    "    of the form (m,d) which means that once t (number of epoch) reaches m, the lr is multiplie\n",
    "    by d. So, if initial lr is 0.1, and steps is [(10,.5),(20,.1),(30,.1)] then the lr\n",
    "    will be 0.1 for epochs 0,..,9; 0.05 for epochs 10,..,19; 0.005 for 20,..,29; and .0005 for \n",
    "    epochs 29 and on\n",
    "    '''\n",
    "    \n",
    "    for n in range(len(steps)):\n",
    "        if t==steps[n][0]:\n",
    "            return lr*steps[n][1]\n",
    "    return lr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(W,X):\n",
    "    '''\n",
    "    Compute the scores of the classes on data in X\n",
    "    Inputs:\n",
    "        W (array): (d,C) matrix of parameters for the C classes\n",
    "        X (array): (N,d) data matrix of features; may or may not include the constant feature\n",
    "    Outputs:\n",
    "        scores (array): (N,C) matrix of scores for each of the N examples and C classes\n",
    "    '''\n",
    "   \n",
    "    if W.shape[0] == X.shape[1]+1: # need to add the constant feature\n",
    "        X=np.hstack((X,np.ones((X.shape[0],1))))\n",
    "    return np.dot(X,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(scores):\n",
    "    '''\n",
    "    IMPLEMENT THIS\n",
    "    Compute the class posterior probabilities from the scores\n",
    "    Inputs:\n",
    "        scores (array): output of scores()\n",
    "    Outputs:\n",
    "        yhat (array): (N,C) matrix of class posteriors for each of the N examples and C classes\n",
    "    Note: you may want to include the dynamic range shift trick discussed in class\n",
    "    '''\n",
    "    N = len(scores)\n",
    "    C = len(scores[0])\n",
    "    yhat = np.zeros((N, C))\n",
    "    \n",
    "      #softmax shift hasnt done yet\n",
    "    # for i in range(N):\n",
    "    #     maxEle = max(scores[i])\n",
    "\n",
    "    #     # prevent overfolow\n",
    "    #     each_row = list(map(lambda x: math.exp(x - maxEle), scores[i]))\n",
    "    #     denominator = sum(each_row)\n",
    "    #     for j in range(C):\n",
    "    #       yhat[i][j] = each_row[j]/denominator\n",
    "  \n",
    "    for i in range(N):\n",
    "        a = max(scores[i])\n",
    "        row = list(map(lambda x:math.exp(x - a), scores[i]))\n",
    "        denominator = sum(row)\n",
    "        for j in range(C):\n",
    "            yhat[i][j] = math.exp(scores[i][j] - a) / denominator\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(y, num_values=10):\n",
    "    \"\"\"\n",
    "    Expects a numpy array of labels like [4, 3, 0, ...]\n",
    "    Outputs: one-hot encoding, e.g. for the above, assuming 10 classes (possible label values)\n",
    "        [\n",
    "            [0,0,0,0,1,0,0,0,0,0],\n",
    "            [0,0,0,1,0,0,0,0,0,0],\n",
    "            [1,0,0,0,0,0,0,0,0,0],\n",
    "            ....\n",
    "        ]\n",
    "    \"\"\"\n",
    "    one_hot_labels = np.zeros((y.size, num_values))\n",
    "    one_hot_labels[np.arange(y.size), y] = 1\n",
    "    return one_hot_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikelihood(yhat,y):\n",
    "    '''\n",
    "    IMPLEMENT THIS\n",
    "    Compute the log-likelihood of the model which produced posterior in yhat corresponding to true labels in y\n",
    "    Inputs:\n",
    "        yhat (array): (N,C) posterior matrix like that returned by softmax()\n",
    "        y (array): (N,C) true labels in one-hot format, or\n",
    "                   (N,) class labels -- your choice (or the function can handle both)\n",
    "    Outputs:\n",
    "        ll (float): the average log-likelihood \n",
    "    '''\n",
    "    N = len(yhat)\n",
    "    C = len(yhat[0])\n",
    "\n",
    "    res = 0\n",
    "    for i in range(N):\n",
    "        res += math.log(yhat[i][y[i]])\n",
    "    res = res / N\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 0, 3, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = list(np.random.randint(10, size = 5))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trainSoftmax(X,Y,params):\n",
    "    '''\n",
    "    IMPLEMENT THIS\n",
    "    Inputs:\n",
    "        X (array): a (N,d) data matrix of features. Does NOT include the constant feature\n",
    "                   (you need to add it if you want to use it)\n",
    "        Y (array): a (N,) vector of class labels\n",
    "        params (dictionary): specification for hyper-parameters. Must include at least the following:\n",
    "          'num_epochs' : max number of epochs for training;\n",
    "          'lr' : the (initial) learning rate;\n",
    "          'scheduler': the function that takes (lr,e,arg) and updates the lr at the end of epoch e;\n",
    "          'lr_update': the arg to pass to scheduler;\n",
    "          'batch_size': the size of the mini-batches to use in SGD\n",
    "          \n",
    "    Outputs:\n",
    "        W (array): an (d,C) matrix of parameters\n",
    "        NLL: the list of values of negative average log-likelihood of the softmax model on the training data, \n",
    "             one number for each training epoch covered.\n",
    "    '''\n",
    "    print(\"hahha\")\n",
    "    num_epochs = params['num_epochs']\n",
    "    lr = params['lr']\n",
    "    scheduler = params['scheduler']\n",
    "    lr_update = params['lr_update']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    epoch = 0\n",
    "    N = len(X)\n",
    "    d = len(X[0])\n",
    "\n",
    "    W = np.zeros((d, N))\n",
    "    NLL = np.zeros(num_epochs)\n",
    "\n",
    "    while (epoch < num_epochs):\n",
    "        if (batch_size > N):\n",
    "            print(\"ERROR: Batch Size larger than sample size\")\n",
    "            exit(-1)\n",
    "        \n",
    "        avg_grad = 0\n",
    "        idx = random.choices(range(N), k = batch_size) \n",
    "        #idx = list(np.random.randint(N, size = batch_size))\n",
    "        for i in idx: # x is 1 x d\n",
    "            y = Y[i]\n",
    "            row_score = np.matmul(X[i], W) # 10 scores of w * x\n",
    "            m = max(row_score) # max element in x * w\n",
    "            denominator = sum(list(map(lambda a:math.exp(a - m), row_score)))\n",
    "\n",
    "            single_score = np.dot(X[i], W[:,y]) # top one score of w_c * x\n",
    "            numerator = X[i] * math.exp(single_score - m)\n",
    "            \n",
    "            grad = - X[i] + numerator / denominator\n",
    "            avg_grad += grad\n",
    "            W[:, y] -= lr * grad # maybe wrong \n",
    "\n",
    "        # for i in idx:\n",
    "\n",
    "        #     # perform gradient descent\n",
    "        #     # calculate gradient at first, according to the formula derived in\n",
    "        #     # Problem 5, we need X[i], and the softmax probability\n",
    "\n",
    "        #     true_class = Y[i]\n",
    "        #     predict = np.matmul(X[i], W)\n",
    "        #     max_val = max(predict)\n",
    "\n",
    "        #     each_row = list(map(lambda x: math.exp(x - max_val), predict))\n",
    "        #     numerator = each_row[true_class]\n",
    "        #     denominator = sum(each_row)\n",
    "\n",
    "        #     grad = X[i] - X[i] * (numerator/denominator)\n",
    "        #     avg_grad += (grad / batch_size)\n",
    "        \n",
    "        #     W[:, true_class] -= lr * avg_grad\n",
    "            \n",
    "        scores_mat = scores(W, X)\n",
    "        yhat = softmax(scores_mat)\n",
    "\n",
    "        #y = convert_to_one_hot(Y, num_values=10)\n",
    "        NLL[epoch] = logLikelihood(yhat, Y)\n",
    "        \n",
    "        \n",
    "        lr = stepSchedule(lr,epoch, lr_update)\n",
    "\n",
    "        epoch += 1\n",
    "        print(\"epoch number:{}\".format(epoch))\n",
    "        \n",
    "    return W,NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of setting the params list to pass to the training code; add your own elements that\n",
    "# specify stopping criteria and anything else\n",
    "params={'lr':.1, 'num_epochs':40,'scheduler': stepSchedule,'batch_size':200,'lr_update':[(20,.5),(30,.1)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the code, follow the problem set assignment, run the experiments, and report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hahha\n"
     ]
    }
   ],
   "source": [
    "W, NLL = trainSoftmax(trainX, trainy, params)\n",
    "predict = softmax(scores(W, testX))\n",
    "N = len(testX)\n",
    "y_predict = np.zeros(N)\n",
    "for i in range(N):\n",
    "    y_predict[i] = np.argmax(predict[i])\n",
    "\n",
    "acc = compute_accuracy(y_predict, testy)\n",
    "print(\"acc\", acc)\n",
    "print(NLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, NLL = trainSoftmax(trainX, trainy, params)\n",
    "predict = softmax(scores[W, valX])\n",
    "N = len(valX)\n",
    "y_predict = np.zeros(N)\n",
    "for i in range(y_predict):\n",
    "    y_predict[i] = np.argmax(predict[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2a4012fca6975ccb4e2908886d96de946d5fe4261c9ff5ee375ddfd35668e36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
