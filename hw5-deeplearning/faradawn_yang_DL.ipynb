{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNM_7_fAwl84"
      },
      "source": [
        "# TTIC 31020 Introduction to Statistical Machine Learning: Neural Networks\n",
        "---\n",
        "Collborated with Sam Zheng, Haichuan Wang\n",
        "\n",
        "In this notebook you will perform classification on the Fashion MNIST dataset with neural networks. Your task is (mostly) to implement the forward and backward methods for different layers (forward methods compute a layer's output given its input, while backward methods compute gradients for its parameters and its input given the gradient of its output).\n",
        "\n",
        "After filling the missing code, try to achieve the best performance by changing the hyperparameters. Neural networks are typically more hyperparameter-sensitive than other methods you've seen in the past homeworks, so good hyperparameter tuning is crucial to get good results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXZjx1F6xBG6",
        "outputId": "67d62a7a-42ff-4956-c92e-e6d2ea5fb807"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35VKVFGLwl89",
        "outputId": "24b57455-ad4f-4ac4-972c-aa48ef9f7786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "/content/drive/MyDrive/Colab Notebooks/TTIC 31020/hw5-dl\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "\n",
        "%autoreload 2\n",
        "\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/TTIC 31020/hw5-dl'\n",
        "\n",
        "import numpy as np\n",
        "from utils import *\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G5nLtNYdwl8_"
      },
      "outputs": [],
      "source": [
        "# superclass of neural network \"modules\" (layers)\n",
        "class Module:\n",
        "    \"\"\"\n",
        "    Module is a super class. It could be a single layer, or a multilayer perceptron.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.train = True\n",
        "        return\n",
        "    \n",
        "    def forward(self, _input):\n",
        "        \"\"\"\n",
        "        z = f(a); a is the input, and h is the output.\n",
        "        \n",
        "        Inputs:\n",
        "        _input: a\n",
        "        \n",
        "        Returns:\n",
        "        output z\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, _input, _gradOutput):\n",
        "        \"\"\"\n",
        "        Compute:\n",
        "        gradient w.r.t. _input\n",
        "        gradient w.r.t. trainable parameters\n",
        "        \n",
        "        Inputs (in lecture notation):\n",
        "        _input: a \n",
        "        _gradOutput: dL/dz\n",
        "        \n",
        "        Returns:\n",
        "        gradInput: dL/dz\n",
        "        \"\"\"\n",
        "        pass\n",
        "        \n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        Return the value of trainable parameters and its corresponding gradient (Used for grandient descent)\n",
        "        \n",
        "        Returns:\n",
        "        params, gradParams\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def training(self):\n",
        "        \"\"\"\n",
        "        Turn the module into training mode.(Only useful for Dropout layer)\n",
        "        Ignore it if you are not using Dropout.\n",
        "        \"\"\"\n",
        "        self.train = True\n",
        "        \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Turn the module into evaluate mode.(Only useful for Dropout layer)\n",
        "        Ignore it if you are not using Dropout.\n",
        "        \"\"\"\n",
        "        self.train = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QNBEfOk1wl9A"
      },
      "outputs": [],
      "source": [
        "# a class representing a sequence of modules (a layered network)\n",
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "    Sequential provides a way to plug layers together in a feed-forward manner.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        Module.__init__(self)\n",
        "        self.layers = [] # layers contain all the layers in order\n",
        "    \n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer) # Add another layer at the end\n",
        "    \n",
        "    def size(self):\n",
        "        return len(self.layers) # How many layers.\n",
        "    \n",
        "    def forward(self, _input):\n",
        "        \"\"\"\n",
        "        Feed forward through all the layers, and return the output of the last layer\n",
        "        \"\"\"\n",
        "        for i in range(self.size()):\n",
        "            # The output of (i-1)-th layer is the _input of i-th layer\n",
        "            _input = self.layers[i].forward(_input)\n",
        "        self._output = _input\n",
        "        return self._output\n",
        "    \n",
        "    def backward(self, _gradOutput):\n",
        "        \"\"\"\n",
        "        Backpropagate through all the layers using chain rule.\n",
        "        \"\"\"\n",
        "        for i in reversed(range(self.size())):\n",
        "            # The (i-1)-th layer receives the error from the i-th layer\n",
        "            _gradOutput = self.layers[i].backward(_gradOutput)\n",
        "        return _gradOutput\n",
        "    \n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        Return trainable parameters and its corresponding gradient in a list\n",
        "        \"\"\"\n",
        "        params = []\n",
        "        gradParams = []\n",
        "        for m in self.layers:\n",
        "            p, g = m.parameters()\n",
        "            if p is not None:\n",
        "                for _p, _g in zip(p,g):\n",
        "                    params.append(_p)\n",
        "                    gradParams.append(_g)\n",
        "        return params, gradParams\n",
        "\n",
        "    def training(self):\n",
        "        \"\"\"\n",
        "        Turn all the layers into training mode\n",
        "        \"\"\"\n",
        "        Module.training(self)\n",
        "        for m in self.layers:\n",
        "            m.training()\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Turn all the layers into evaluate mode\n",
        "        \"\"\"\n",
        "        Module.evaluate(self)\n",
        "        for m in self.layers:\n",
        "            m.evaluate()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "B2RUG0wmwl9B"
      },
      "outputs": [],
      "source": [
        "class FullyConnected(Module):\n",
        "    \"\"\"\n",
        "    Fully connected layer (parameters include a matrix of weights a vector of biases)\n",
        "    \"\"\"\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        Module.__init__(self)\n",
        "        \n",
        "        # ADD CODE here to initialize the weights and biases\n",
        "        self.weight = np.random.rand(outputSize, inputSize) * 0.2 - 0.1\n",
        "        self.bias = np.random.rand(outputSize) * 0.2 - 0.1\n",
        "        \n",
        "        self.gradWeight = np.ndarray((inputSize, outputSize))\n",
        "        self.gradBias = np.ndarray(outputSize)\n",
        "        \n",
        "    def forward(self, _input):\n",
        "        \"\"\"\n",
        "        output = W * input + b\n",
        "        \"\"\"\n",
        "        N = len(_input)\n",
        "        self._input = _input\n",
        "        self._output = np.matmul(self.weight, _input.T).T # ADD CODE here, 3 x 8\n",
        "        for i in range(N):\n",
        "            self._output[i] += self.bias\n",
        "\n",
        "        # print(\"Fully forward self._output\", self._output)\n",
        "        return self._output\n",
        "    \n",
        "    def backward(self, _gradOutput):\n",
        "        \"\"\"\n",
        "        gradWeight = gradOutput * input\n",
        "        gradBias = gradOutput * vec(1)\n",
        "        gradInput =  Weight * gradOutput\n",
        "        \"\"\"\n",
        "        self.gradWeight.fill(0)\n",
        "        self.gradBias.fill(0)\n",
        "        self.gradWeight += np.matmul(self._input.T, _gradOutput) # ADD CODE to compute the gradient for the layer's weight\n",
        "        self.gradBias += np.matmul(_gradOutput.T, np.ones(len(_gradOutput))) # ADD CODE to compute the gradient for the layer's bias\n",
        "        self._gradInput = np.matmul(_gradOutput, self.weight)  # ADD CODE to compute the gradient for the layer's input\n",
        "\n",
        "        return self._gradInput\n",
        "        \n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        Return weight and bias and their g\n",
        "        \"\"\"\n",
        "        return [self.weight, self.bias], [self.gradWeight, self.gradBias]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1eqHFJA8wl9C"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    \"\"\"\n",
        "    ReLU activation, not trainable.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        Module.__init__(self)\n",
        "        return\n",
        "    \n",
        "    def forward(self, _input):\n",
        "        \"\"\"\n",
        "        output = max(0, input)\n",
        "        \"\"\"\n",
        "        self._input = _input\n",
        "        self._output = _input\n",
        "        N = len(_input)\n",
        "        for i in range(N):\n",
        "            self._output[i] = np.maximum(np.zeros(len(_input[i])), _input[i]) # ADD CODE to compute the layer's output\n",
        "        return self._output\n",
        "    \n",
        "    def backward(self, _gradOutput):\n",
        "        \"\"\"\n",
        "        gradInput = gradOutput * mask\n",
        "        mask = _input > 0\n",
        "        \"\"\"\n",
        "        mask = self._input > 0\n",
        "        self._gradInput =  _gradOutput * mask # ADD CODE to compute the gradient for the layer's input\n",
        "        return self._gradInput\n",
        "        \n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        No trainable parametersm, return None\n",
        "        \"\"\"\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NHcZ_CwWwl9D"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "    \"\"\"\n",
        "    A dropout layer\n",
        "    \"\"\"\n",
        "    def __init__(self, p = 0.5):\n",
        "        Module.__init__(self)\n",
        "        self.p = p #self.p is the drop rate, if self.p is 0, then it's a identity layer\n",
        "        \n",
        "    def forward(self, _input):\n",
        "        self._output = _input\n",
        "        if self.p > 0:\n",
        "            if self.train:\n",
        "                # Randomize a mask from bernoulli distrubition\n",
        "                self.mask = np.random.binomial(1, 1 - self.p, _input.shape).astype('float64')\n",
        "                # Scale the mask\n",
        "                self.mask /= 1 - self.p\n",
        "                self._output *= self.mask\n",
        "        return self._output\n",
        "    \n",
        "    def backward(self, _gradOutput):\n",
        "        self._gradInput = _gradOutput\n",
        "        if self.train:\n",
        "            if self.p > 0:\n",
        "                self._gradInput = self.mask* self._gradInput \n",
        "        return self._gradInput\n",
        "    \n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        No trainable parameters.\n",
        "        \"\"\"\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RLdzchKlwl9D"
      },
      "outputs": [],
      "source": [
        "class SoftMaxLoss(object):\n",
        "    def __init__(self):\n",
        "        return\n",
        "        \n",
        "    def forward(self, _input, _label):\n",
        "        \"\"\"\n",
        "        Softmax and cross entropy loss layer. Should return a scalar, since it's a\n",
        "        loss. (It's almost identical to what we had in Pset 2)\n",
        "\n",
        "        Inputs:\n",
        "        _input: N x C\n",
        "        _labels: N x C, one-hot\n",
        "\n",
        "        Returns: loss (scalar)\n",
        "        \"\"\"\n",
        "        self._input = _input - _input.max(1)[:, np.newaxis]\n",
        "        self._logprob = self._input - np.log(np.exp(self._input).sum(1)[:, np.newaxis])\n",
        "        \n",
        "        self._output = np.mean(np.sum(-self._logprob * _label, 1))\n",
        "        return self._output\n",
        "    \n",
        "    def backward(self, _label):\n",
        "        # score = np.exp(self._input)\n",
        "        # N = len(self._input)\n",
        "        # C = len(self._input[0])\n",
        "        # self._gradInput = np.zeros((N, C))\n",
        "        # for i in range(N):\n",
        "        #     c = np.argmax(_label[i])\n",
        "        #     self._gradInput[i] = 1 / sum(score[i]) * score[i] / N # ADD CODE to compute the gradient for the layer's input\n",
        "        #     self._gradInput[i][c] -= 1 / N\n",
        "        self._gradInput = (np.exp(self._logprob) - _label) / len(self._input)\n",
        "        return self._gradInput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRQRMqV0wl9E",
        "outputId": "87ab11e7-d480-4af9-e415-bbaae884fce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.345378329122795\n",
            "4.9687117176946155e-09\n"
          ]
        }
      ],
      "source": [
        "# Test softmaxloss, the relative error should be small enough\n",
        "def test_sm():\n",
        "    crit = SoftMaxLoss()\n",
        "    gt = np.zeros((3, 10))\n",
        "    gt[np.arange(3), np.array([1,2,3])] = 1\n",
        "    x = np.random.random((3,10))\n",
        "    def test_f(x):\n",
        "        return crit.forward(x, gt)\n",
        "\n",
        "    print(crit.forward(x, gt))\n",
        "    gradInput = crit.backward(gt)\n",
        "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
        "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
        "    \n",
        "test_sm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYnn3fJqwl9F",
        "outputId": "4e260f36-f2be-4f8b-b330-58d64c6bcf0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing FullyConnected\n",
            "3.4881282650908704e-09\n",
            "testing ReLU\n",
            "5.962448038856628e-10\n",
            "testing Dropout\n",
            "7.360228254783035e-10\n"
          ]
        }
      ],
      "source": [
        "# Test modules, all the relative errors should be small enough (on the order of 1e-6 or smaller)\n",
        "def test_module(model):\n",
        "    model.evaluate()\n",
        "\n",
        "    crit = TestCriterion()\n",
        "    gt = np.random.random((3,10)) # what is gt and x\n",
        "    x = np.random.random((3,10)) # what is gt and x\n",
        "    def test_f(x):\n",
        "        return crit.forward(model.forward(x), gt)\n",
        "\n",
        "    test_f(x)\n",
        "    gradInput = model.backward(crit.backward(gt))\n",
        "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
        "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
        "\n",
        "# Test fully connected\n",
        "model = FullyConnected(10, 8)\n",
        "print('testing FullyConnected')\n",
        "test_module(model)\n",
        "\n",
        "# Test ReLU\n",
        "model = ReLU()\n",
        "print('testing ReLU')\n",
        "test_module(model)\n",
        "\n",
        "# Test Dropout\n",
        "model = Dropout()\n",
        "print('testing Dropout')\n",
        "test_module(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "c5yjXQAtwl9G"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "#ADD CODE to add layers using the add attribute of sequential \n",
        "#to construct 2-layer Neural Network with hidden size 10\n",
        "model.add(FullyConnected(10, 8))\n",
        "model.add(ReLU())\n",
        "model.add(FullyConnected(8, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GajIsVR_wl9G",
        "outputId": "c8f9b9f2-33dc-48a3-f83b-3734e08e1756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing 2-layer model\n",
            "3.7584494041918615e-08\n"
          ]
        }
      ],
      "source": [
        "# Test your neural network\n",
        "print('testing 2-layer model')\n",
        "test_module(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xRkoRHNxwl9H"
      },
      "outputs": [],
      "source": [
        "def sgd(x, dx, lr, weight_decay = 0):\n",
        "    num_layers = len(x) // 2\n",
        "    for i in range(num_layers):\n",
        "        x[2*i] -= lr * (dx[2*i].T + 2 * weight_decay * x[2*i])\n",
        "        x[2*i + 1] -= lr * dx[2*i + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgHeqPh7wl9H",
        "outputId": "ddaa7375-7afc-4c48-ac89-4e84dad22c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.27641112127585904\n",
            "loss 0.02191976659445667\n",
            "loss 0.022571379324979324\n",
            "loss 0.023062557513587597\n",
            "loss 0.02342705905966593\n",
            "loss 0.023691929205351403\n",
            "loss 0.02342610590521886\n",
            "loss 0.02975577013499246\n",
            "loss 0.036383738739548484\n",
            "loss 0.03654155092476587\n",
            "loss 0.036659122030493575\n"
          ]
        }
      ],
      "source": [
        "# Test gradient descent, the loss should be lower and lower\n",
        "model = Sequential()\n",
        "model.add(FullyConnected(10, 8))\n",
        "model.add(ReLU())\n",
        "model.add(FullyConnected(8, 5))\n",
        "\n",
        "trainX = np.random.random((3,10))\n",
        "\n",
        "crit = TestCriterion()\n",
        "\n",
        "params, gradParams = model.parameters()\n",
        "\n",
        "it = 0\n",
        "state = None\n",
        "while True:\n",
        "    # print(\"==== Iter\", it)\n",
        "    output = model.forward(trainX)\n",
        "    loss = crit.forward(output, None)\n",
        "    if it % 100 == 0:\n",
        "        print(\"loss\", loss)\n",
        "    doutput = crit.backward(None)\n",
        "    model.backward(doutput)\n",
        "    \n",
        "    sgd(params, gradParams, 0.01)\n",
        "    # print(\"params after sgd\", params[0][0][0])\n",
        "    params1, gradParams1 = model.parameters()\n",
        "    # print(\"params in the model\", params1[0][0][0])\n",
        "    \n",
        "    if it > 1000:\n",
        "        break\n",
        "    it += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUB9hq4Rwl9H"
      },
      "source": [
        "Now we start to work on Fashion MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBSmrFCjwl9I",
        "outputId": "687cf354-ebe5-424b-a932-4d8ddb2aedbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load large trainset.\n",
            "(50000, 784)\n",
            "(50000, 10)\n",
            "Load valset.\n",
            "(5000, 784)\n",
            "(5000, 10)\n"
          ]
        }
      ],
      "source": [
        "import FMNIST_utils\n",
        "\n",
        "# We only consider large set this time\n",
        "print(\"Load large trainset.\")\n",
        "Xlarge,Ylarge = FMNIST_utils.load_data(\"Tr\")\n",
        "print(Xlarge.shape)\n",
        "print(Ylarge.shape)\n",
        "if Xlarge.max() > 1: Xlarge = Xlarge/255\n",
        "\n",
        "print(\"Load valset.\")\n",
        "Xval,Yval = FMNIST_utils.load_data(\"Vl\")\n",
        "print(Xval.shape)\n",
        "print(Yval.shape)\n",
        "if Xval.max() > 1: Xval = Xval/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7s_4EP66wl9I"
      },
      "outputs": [],
      "source": [
        "def predict(X, model):\n",
        "    \"\"\"\n",
        "    Evaluate the soft predictions of the model.\n",
        "    Input:\n",
        "    X : N x d array (no unit terms)\n",
        "    model : a multi-layer perceptron\n",
        "    Output:\n",
        "    yhat : N x C array\n",
        "        yhat[n][:] contains the score over C classes for X[n][:]\n",
        "    \"\"\"\n",
        "    return model.forward(X)\n",
        "\n",
        "def error_rate(X, Y, model):\n",
        "    \"\"\"\n",
        "    Compute error rate (between 0 and 1) for the model\n",
        "    \"\"\"\n",
        "    model.evaluate()\n",
        "    res = 1 - (model.forward(X).argmax(-1) == Y.argmax(-1)).mean()\n",
        "    model.training()\n",
        "    return res\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "def runTrainVal(X,Y,model,Xval,Yval,trainopt):\n",
        "    \"\"\"\n",
        "    Run the train + evaluation on a given train/val partition\n",
        "    trainopt: various (hyper)parameters of the training procedure\n",
        "    During training, choose the model with the lowest validation error. (early stopping)\n",
        "    Assumes (global) variable crit containing the loss (training \"criterion\" to be minimized)\n",
        "    \"\"\"\n",
        "    \n",
        "    params, gradParams = model.parameters()\n",
        "    \n",
        "    eta = trainopt['eta']\n",
        "    \n",
        "    N = X.shape[0] # number of data points in X\n",
        "    \n",
        "    # Save the model with lowest validation error\n",
        "    minValError = np.inf\n",
        "    saved_model = None # Save the best model accoring to validation error\n",
        "    \n",
        "    shuffled_idx = np.random.permutation(N)\n",
        "    start_idx = 0\n",
        "    for iteration in range(trainopt['maxiter']):\n",
        "        if iteration % int(trainopt['eta_frac'] * trainopt['maxiter']) == 0:\n",
        "            eta *= trainopt['etadrop']\n",
        "        # form the next mini-batch\n",
        "        stop_idx = min(start_idx + trainopt['batch_size'], N)\n",
        "        batch_idx = range(N)[int(start_idx):int(stop_idx)]\n",
        "        \n",
        "        s_idx = shuffled_idx[batch_idx]\n",
        "        \n",
        "        bX = X[s_idx,:]\n",
        "        bY = Y[s_idx,:]\n",
        "\n",
        "        score = model.forward(bX)\n",
        "        loss = crit.forward(score, bY)\n",
        "        # note: this computes loss on the *batch* only, not on the entire training set!\n",
        "        \n",
        "        dscore = crit.backward(bY)\n",
        "        model.backward(dscore)\n",
        "        \n",
        "        sgd(params, gradParams, eta, weight_decay = trainopt['lambda'])\n",
        "\n",
        "        start_idx = stop_idx % N\n",
        "        \n",
        "        if (iteration % trainopt['display_iter']) == 0:\n",
        "            #compute train and val error; multiply by 100 for readability (make it percentage points)\n",
        "            trainError = 100 * error_rate(X, Y, model)\n",
        "            valError = 100 * error_rate(Xval, Yval, model)\n",
        "            print('{:8} batch loss: {:.3f} train error: {:.3f} val error: {:.3f}'.format(iteration, loss, trainError, valError))\n",
        "            \n",
        "            # early stopping: save the best model snapshot so far (i.e., model with lowest val error)\n",
        "            if valError < minValError:\n",
        "                saved_model = deepcopy(model)\n",
        "                minValError = valError\n",
        "        \n",
        "    return saved_model, minValError, trainError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SEjVqLDUwl9I"
      },
      "outputs": [],
      "source": [
        "def build_model(input_size, hidden_size, output_size, activation_func = 'ReLU', dropout = 0):\n",
        "    \"\"\"\n",
        "    Build a 2-layer model:\n",
        "    input_size: the dimension of input data\n",
        "    hidden_size: the dimension of hidden vector, hidden_size == 0 means only one layer\n",
        "    output_size: the output size of final layer.\n",
        "    activation_func: ReLU, sigmoid (defined above), Tanh (you'd have to define), etc. \n",
        "    dropout: the dropout rate: if dropout == 0, this is equivalent to no dropout\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    \n",
        "    if type(hidden_size) is int:\n",
        "        hidden_size = [hidden_size] # ensure it's a list\n",
        "    \n",
        "    prev_size=input_size\n",
        "    \n",
        "    # add hidden layer(s) as requested\n",
        "    if hidden_size[0] == 0: # no hidden layer\n",
        "        pass\n",
        "    \n",
        "    else:\n",
        "        for l in range(len(hidden_size)):\n",
        "            # ADD CODE to add a fully connected layer \n",
        "            model.add(FullyConnected(prev_size, hidden_size[l]))\n",
        "            \n",
        "            prev_size=hidden_size[l]\n",
        "\n",
        "            # ADD CODE to add a Relu\n",
        "            model.add(ReLU())\n",
        "                 \n",
        "            if dropout > 0:\n",
        "                model.add(Dropout(p = dropout))\n",
        "            \n",
        "                \n",
        "                \n",
        "    # ADD CODE to add output layer  (which is a fully connected layer)\n",
        "    model.add(FullyConnected(prev_size, output_size))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "spaCYVFOwl9I"
      },
      "outputs": [],
      "source": [
        "trainopt = {\n",
        "    'eta': 1e-3,   # initial learning rate\n",
        "    'maxiter': 20000,   # max number of iterations (updates) of SGD\n",
        "    'display_iter': 5000,  # display batch loss every display_iter updates\n",
        "    'batch_size': 128,  \n",
        "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
        "    'eta_frac': .4,  # drop eta after every eta_frac*maxiter\n",
        "    'update': 'sgd'\n",
        "}\n",
        "NFEATURES = Xlarge.shape[1]\n",
        "\n",
        "# we will maintain a record of models trained for different values of lambda\n",
        "# these will be indexed directly by lambda value itself\n",
        "trained_models = dict()\n",
        "\n",
        "lambda_ = 0.01\n",
        "hidden_size_ = [20]\n",
        "trainopt['lambda'] = lambda_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFpxGSbxwl9J",
        "outputId": "f341e086-1285-4784-e606-a0c3a8609992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== hidden_size [5]\n",
            "       0 batch loss: 2.305 train error: 91.056 val error: 91.180\n",
            "    5000 batch loss: 2.159 train error: 76.266 val error: 76.840\n",
            "   10000 batch loss: 2.032 train error: 63.268 val error: 64.220\n",
            "   15000 batch loss: 2.041 train error: 58.972 val error: 60.040\n",
            "train set model [ h = 5  ], lambda= 0.0100 ] --> train error: 58.97, val error: 60.04\n",
            "=== hidden_size [20]\n",
            "       0 batch loss: 2.332 train error: 86.644 val error: 87.420\n",
            "    5000 batch loss: 1.759 train error: 51.280 val error: 51.940\n",
            "   10000 batch loss: 1.620 train error: 40.002 val error: 39.760\n",
            "   15000 batch loss: 1.427 train error: 37.330 val error: 37.660\n",
            "train set model [ h = 20  ], lambda= 0.0100 ] --> train error: 37.33, val error: 37.66\n",
            "=== hidden_size [100]\n",
            "       0 batch loss: 2.393 train error: 86.934 val error: 86.940\n",
            "    5000 batch loss: 1.452 train error: 35.566 val error: 35.880\n",
            "   10000 batch loss: 1.245 train error: 32.576 val error: 32.460\n",
            "   15000 batch loss: 1.150 train error: 31.506 val error: 31.340\n",
            "train set model [ h = 100  ], lambda= 0.0100 ] --> train error: 31.51, val error: 31.34\n",
            "=== hidden_size [200]\n",
            "       0 batch loss: 2.468 train error: 94.758 val error: 94.620\n",
            "    5000 batch loss: 1.172 train error: 31.606 val error: 31.620\n",
            "   10000 batch loss: 0.994 train error: 27.864 val error: 27.440\n",
            "   15000 batch loss: 1.038 train error: 26.472 val error: 25.840\n",
            "train set model [ h = 200  ], lambda= 0.0100 ] --> train error: 26.47, val error: 25.84\n",
            "=== hidden_size [50]\n",
            "       0 batch loss: 2.302 train error: 84.072 val error: 83.300\n",
            "    5000 batch loss: 1.657 train error: 37.646 val error: 38.400\n",
            "   10000 batch loss: 1.341 train error: 34.406 val error: 34.340\n",
            "   15000 batch loss: 1.239 train error: 33.124 val error: 32.600\n",
            "train set model [ h = 50  ], lambda= 0.0100 ] --> train error: 33.12, val error: 32.60\n",
            "=== hidden_size [50, 50]\n",
            "       0 batch loss: 2.306 train error: 88.102 val error: 87.760\n",
            "    5000 batch loss: 2.215 train error: 76.712 val error: 76.740\n",
            "   10000 batch loss: 2.057 train error: 44.746 val error: 43.920\n",
            "   15000 batch loss: 1.890 train error: 43.894 val error: 43.720\n",
            "train set model [ h = 50 50  ], lambda= 0.0100 ] --> train error: 43.89, val error: 43.72\n",
            "=== hidden_size [50, 50, 50]\n",
            "       0 batch loss: 2.307 train error: 90.036 val error: 90.120\n",
            "    5000 batch loss: 2.304 train error: 90.034 val error: 90.120\n",
            "   10000 batch loss: 2.297 train error: 89.792 val error: 90.000\n",
            "   15000 batch loss: 2.282 train error: 88.588 val error: 88.520\n",
            "train set model [ h = 50 50 50  ], lambda= 0.0100 ] --> train error: 88.59, val error: 88.52\n",
            "=== hidden_size [50, 50, 50, 50]\n",
            "       0 batch loss: 2.310 train error: 90.028 val error: 90.060\n",
            "    5000 batch loss: 2.304 train error: 88.924 val error: 88.760\n",
            "   10000 batch loss: 2.305 train error: 86.714 val error: 86.680\n",
            "   15000 batch loss: 2.299 train error: 86.294 val error: 86.100\n",
            "train set model [ h = 50 50 50 50  ], lambda= 0.0100 ] --> train error: 86.29, val error: 86.10\n"
          ]
        }
      ],
      "source": [
        "# Tuning hidden layer width and dpeth\n",
        "\n",
        "for hidden_size_ in [[5], [20], [100], [200], [50], [50, 50], [50, 50, 50], [50, 50, 50, 50]]:\n",
        "    model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.5) \n",
        "    print(\"=== hidden_size\", hidden_size_)\n",
        "    trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
        "    trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
        "    print('train set model [ h = ',end='')\n",
        "    for l in range(len(hidden_size_)):\n",
        "        print('%d '%hidden_size_[l],end='')\n",
        "    print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
        "\n",
        "# Observation 1: the larger the hidden layer width, the better the performance.\n",
        "# This might be that FMNIST data set has many features. Thus, more units allows\n",
        "# each feature to be represented and tuned.\n",
        "# Yet, too much hidden units might lead to over-fitting. \n",
        "# And increasing the size of one layer alone doesn't change the number of activation function.\n",
        "\n",
        "# Observation 2: depth of the network doesn't enhance the performance as much.\n",
        "# A shallow but wide network can approximate any function.\n",
        "# Adding depth allows the model to tune features at different level of abstractions.\n",
        "# For this FMNIST dataset, perhaps there are not many levels of abstractions.\n",
        "# Therefore, width is more effective than depth. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best model\n",
        "# The most influential parameters are: eta_frac, lambda, hidden_size_,\n",
        "# and initial weight (-0.1 to 0.1, better than 0.001)\n",
        "\n",
        "trainopt = {\n",
        "    'eta': 1e-3,   # initial learning rate\n",
        "    'maxiter': 20000,   # max number of iterations (updates) of SGD\n",
        "    'display_iter': 5000,  # display batch loss every display_iter updates\n",
        "    'batch_size': 128,  \n",
        "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
        "    'eta_frac': .4,  # drop eta after every eta_frac*maxiter\n",
        "    'update': 'sgd',\n",
        "    'lambda': 0.01\n",
        "}\n",
        "\n",
        "hidden_size_ = [300]\n",
        "\n",
        "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.5) \n",
        "print(\"=== hidden_size\", hidden_size_)\n",
        "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
        "trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
        "print('train set model [ h = ',end='')\n",
        "for l in range(len(hidden_size_)):\n",
        "    print('%d '%hidden_size_[l],end='')\n",
        "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryXF3zZn4eAw",
        "outputId": "cf65893e-2d8f-4249-e203-78b5550d259b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== hidden_size [300]\n",
            "       0 batch loss: 2.646 train error: 95.232 val error: 95.540\n",
            "    5000 batch loss: 1.111 train error: 30.224 val error: 29.920\n",
            "   10000 batch loss: 0.901 train error: 26.180 val error: 26.160\n",
            "   15000 batch loss: 0.897 train error: 24.878 val error: 24.780\n",
            "train set model [ h = 300  ], lambda= 0.0100 ] --> train error: 24.88, val error: 24.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhHk-TMpwl9J",
        "outputId": "91c7bdf8-99bc-4d9f-ef92-d4b6a499ab4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: submission-fmnist.csv\n"
          ]
        }
      ],
      "source": [
        "#Generate a Kaggle submission file using best_trained_model which you should set based on your experiments\n",
        "kaggleX = FMNIST_utils.load_data('kaggle')\n",
        "if kaggleX.max() > 1: kaggleX = kaggleX/255\n",
        "kaggleYhat = predict(kaggleX, trained_model).argmax(-1)\n",
        "save_submission('submission-fmnist.csv', kaggleYhat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MVoS3h1w_f82"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f594f625523eedf37de17555ec746ece4289f4fa39150ea03385bfab3da2cc11"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}