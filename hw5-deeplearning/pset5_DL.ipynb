{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTIC 31020 Introduction to Statistical Machine Learning: Neural Networks\n",
    "---\n",
    "\n",
    "In this notebook you will perform classification on the Fashion MNIST dataset with neural networks. Your task is (mostly) to implement the forward and backward methods for different layers (forward methods compute a layer's output given its input, while backward methods compute gradients for its parameters and its input given the gradient of its output).\n",
    "\n",
    "After filling the missing code, try to achieve the best performance by changing the hyperparameters. Neural networks are typically more hyperparameter-sensitive than other methods you've seen in the past homeworks, so good hyperparameter tuning is crucial to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superclass of neural network \"modules\" (layers)\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Module is a super class. It could be a single layer, or a multilayer perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        z = f(a); a is the input, and h is the output.\n",
    "        \n",
    "        Inputs:\n",
    "        _input: a\n",
    "        \n",
    "        Returns:\n",
    "        output z\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Compute:\n",
    "        gradient w.r.t. _input\n",
    "        gradient w.r.t. trainable parameters\n",
    "        \n",
    "        Inputs (in lecture notation):\n",
    "        _input: a \n",
    "        _gradOutput: dL/dz\n",
    "        \n",
    "        Returns:\n",
    "        gradInput: dL/dz\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return the value of trainable parameters and its corresponding gradient (Used for grandient descent)\n",
    "        \n",
    "        Returns:\n",
    "        params, gradParams\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn the module into training mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn the module into evaluate mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class representing a sequence of modules (a layered network)\n",
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Sequential provides a way to plug layers together in a feed-forward manner.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        self.layers = [] # layers contain all the layers in order\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer) # Add another layer at the end\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.layers) # How many layers.\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        Feed forward through all the layers, and return the output of the last layer\n",
    "        \"\"\"\n",
    "        for i in range(self.size()):\n",
    "            # The output of (i-1)-th layer is the _input of i-th layer\n",
    "            _input = self.layers[i].forward(_input)\n",
    "        self._output = _input\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _gradOutput):\n",
    "        \"\"\"\n",
    "        Backpropagate through all the layers using chain rule.\n",
    "        \"\"\"\n",
    "        for i in reversed(range(self.size())):\n",
    "            # The (i-1)-th layer receives the error from the i-th layer\n",
    "            _gradOutput = self.layers[i].backward(_gradOutput)\n",
    "        return _gradOutput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return trainable parameters and its corresponding gradient in a list\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        gradParams = []\n",
    "        for m in self.layers:\n",
    "            p, g = m.parameters()\n",
    "            if p is not None:\n",
    "                for _p, _g in zip(p,g):\n",
    "                    params.append(_p)\n",
    "                    gradParams.append(_g)\n",
    "        return params, gradParams\n",
    "\n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into training mode\n",
    "        \"\"\"\n",
    "        Module.training(self)\n",
    "        for m in self.layers:\n",
    "            m.training()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into evaluate mode\n",
    "        \"\"\"\n",
    "        Module.evaluate(self)\n",
    "        for m in self.layers:\n",
    "            m.evaluate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer (parameters include a matrix of weights a vector of biases)\n",
    "    \"\"\"\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "        # ADD CODE here to initialize the weights and biases\n",
    "        self.weight = \n",
    "        self.bias = \n",
    "        \n",
    "        self.gradWeight = np.ndarray((inputSize, outputSize))\n",
    "        self.gradBias = np.ndarray(outputSize)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = W * input + b\n",
    "        \"\"\"\n",
    "        self._input = _input\n",
    "        \n",
    "        self._output =   # ADD CODE to compute the layer's output\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradWeight = gradOutput * input\n",
    "        gradBias = gradOutput * vec(1)\n",
    "        gradInput =  gradWeight * gradOutput\n",
    "        \"\"\"\n",
    "        self.gradWeight.fill(0)\n",
    "        self.gradBias.fill(0)\n",
    "        \n",
    "        self.gradWeight += # ADD CODE to compute the gradient for the layer's weight\n",
    "        self.gradBias += # ADD CODE to compute the gradient for the layer's bias\n",
    "        self._gradInput =  # ADD CODE to compute the gradient for the layer's input\n",
    "\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return weight and bias and their g\n",
    "        \"\"\"\n",
    "        return [self.weight, self.bias], [self.gradWeight, self.gradBias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    ReLU activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = max(0, input)\n",
    "        \"\"\"\n",
    "        self._input = _input\n",
    "        self._output =  # ADD CODE to compute the layer's output\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradInput = gradOutput * mask\n",
    "        mask = _input > 0\n",
    "        \"\"\"\n",
    "        self._gradInput =  # ADD CODE to compute the gradient for the layer's input\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parametersm, return None\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    A dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p = 0.5):\n",
    "        Module.__init__(self)\n",
    "        self.p = p #self.p is the drop rate, if self.p is 0, then it's a identity layer\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        self._output = _input\n",
    "        if self.p > 0:\n",
    "            if self.train:\n",
    "                # Randomize a mask from bernoulli distrubition\n",
    "                self.mask = np.random.binomial(1, 1 - self.p, _input.shape).astype('float64')\n",
    "                # Scale the mask\n",
    "                self.mask /= 1 - self.p\n",
    "                self._output *= self.mask\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _gradOutput):\n",
    "        self._gradInput = _gradOutput\n",
    "        if self.train:\n",
    "            if self.p > 0:\n",
    "                self._gradInput = self.mask* self._gradInput \n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parameters.\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxLoss(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def forward(self, _input, _label):\n",
    "        \"\"\"\n",
    "        Softmax and cross entropy loss layer. Should return a scalar, since it's a\n",
    "        loss. (It's almost identical to what we had in Pset 2)\n",
    "\n",
    "        Inputs:\n",
    "        _input: N x C\n",
    "        _labels: N x C, one-hot\n",
    "\n",
    "        Returns: loss (scalar)\n",
    "        \"\"\"\n",
    "        self._input = _input - _input.max(1)[:, np.newaxis]\n",
    "        self._logprob = self._input - np.log(np.exp(self._input).sum(1)[:, np.newaxis])\n",
    "        \n",
    "        self._output = np.mean(np.sum(-self._logprob * _label, 1))\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _label):\n",
    "        self._gradInput =  # ADD CODE to compute the gradient for the layer's input\n",
    "        return self._gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test softmaxloss, the relative error should be small enough\n",
    "def test_sm():\n",
    "    crit = SoftMaxLoss()\n",
    "    gt = np.zeros((3, 10))\n",
    "    gt[np.arange(3), np.array([1,2,3])] = 1\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(x, gt)\n",
    "\n",
    "    print(crit.forward(x, gt))\n",
    "    gradInput = crit.backward(gt)\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "    \n",
    "test_sm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test modules, all the relative errors should be small enough (on the order of 1e-6 or smaller)\n",
    "def test_module(model):\n",
    "    model.evaluate()\n",
    "\n",
    "    crit = TestCriterion()\n",
    "    gt = np.random.random((3,10))\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(model.forward(x), gt)\n",
    "\n",
    "    test_f(x)\n",
    "    gradInput = model.backward(crit.backward(gt))\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "\n",
    "# Test fully connected\n",
    "model = FullyConnected(10, 10)\n",
    "print('testing FullyConnected')\n",
    "test_module(model)\n",
    "\n",
    "# Test ReLU\n",
    "model = ReLU()\n",
    "print('testing ReLU')\n",
    "test_module(model)\n",
    "\n",
    "# Test Dropout\n",
    "model = Dropout()\n",
    "print('testing Dropout')\n",
    "test_module(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#ADD CODE to add layers using the add attribute of sequential \n",
    "#to construct 2-layer Neural Network with hidden size 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your neural network\n",
    "print('testing 2-layer model')\n",
    "test_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x, dx, lr, weight_decay = 0):\n",
    "    for _x, _dx in zip(x, dx):\n",
    "        _x =   #ADD CODE to perform one gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradient descent, the loss should be lower and lower\n",
    "trainX = np.random.random((10,10))\n",
    "\n",
    "\n",
    "\n",
    "crit = TestCriterion()\n",
    "\n",
    "params, gradParams = model.parameters()\n",
    "\n",
    "it = 0\n",
    "state = None\n",
    "while True:\n",
    "    output = model.forward(trainX)\n",
    "    loss = crit.forward(output, None)\n",
    "    if it % 100 == 0:\n",
    "        print(loss)\n",
    "    doutput = crit.backward(None)\n",
    "    model.backward(doutput)\n",
    "    sgd(params, gradParams, 0.01)\n",
    "    if it > 1000:\n",
    "        break\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start to work on Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FMNIST_utils\n",
    "\n",
    "# We only consider large set this time\n",
    "print(\"Load large trainset.\")\n",
    "Xlarge,Ylarge = FMNIST_utils.load_data(\"Tr\")\n",
    "print(Xlarge.shape)\n",
    "print(Ylarge.shape)\n",
    "if Xlarge.max() > 1: Xlarge = Xlarge/255\n",
    "\n",
    "print(\"Load valset.\")\n",
    "Xval,Yval = FMNIST_utils.load_data(\"Vl\")\n",
    "print(Xval.shape)\n",
    "print(Yval.shape)\n",
    "if Xval.max() > 1: Xval = Xval/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"\n",
    "    Evaluate the soft predictions of the model.\n",
    "    Input:\n",
    "    X : N x d array (no unit terms)\n",
    "    model : a multi-layer perceptron\n",
    "    Output:\n",
    "    yhat : N x C array\n",
    "        yhat[n][:] contains the score over C classes for X[n][:]\n",
    "    \"\"\"\n",
    "    return model.forward(X)\n",
    "\n",
    "def error_rate(X, Y, model):\n",
    "    \"\"\"\n",
    "    Compute error rate (between 0 and 1) for the model\n",
    "    \"\"\"\n",
    "    model.evaluate()\n",
    "    res = 1 - (model.forward(X).argmax(-1) == Y.argmax(-1)).mean()\n",
    "    model.training()\n",
    "    return res\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def runTrainVal(X,Y,model,Xval,Yval,trainopt):\n",
    "    \"\"\"\n",
    "    Run the train + evaluation on a given train/val partition\n",
    "    trainopt: various (hyper)parameters of the training procedure\n",
    "    During training, choose the model with the lowest validation error. (early stopping)\n",
    "    Assumes (global) variable crit containing the loss (training \"criterion\" to be minimized)\n",
    "    \"\"\"\n",
    "    \n",
    "    params, gradParams = model.parameters()\n",
    "    \n",
    "    eta = trainopt['eta']\n",
    "    \n",
    "    N = X.shape[0] # number of data points in X\n",
    "    \n",
    "    # Save the model with lowest validation error\n",
    "    minValError = np.inf\n",
    "    saved_model = None # Save the best model accoring to validation error\n",
    "    \n",
    "    shuffled_idx = np.random.permutation(N)\n",
    "    start_idx = 0\n",
    "    for iteration in range(trainopt['maxiter']):\n",
    "        if iteration % int(trainopt['eta_frac'] * trainopt['maxiter']) == 0:\n",
    "            eta *= trainopt['etadrop']\n",
    "        # form the next mini-batch\n",
    "        stop_idx = min(start_idx + trainopt['batch_size'], N)\n",
    "        batch_idx = range(N)[int(start_idx):int(stop_idx)]\n",
    "        \n",
    "        s_idx = shuffled_idx[batch_idx]\n",
    "        \n",
    "        bX = X[s_idx,:]\n",
    "        bY = Y[s_idx,:]\n",
    "\n",
    "        score = model.forward(bX)\n",
    "        loss = crit.forward(score, bY)\n",
    "        # note: this computes loss on the *batch* only, not on the entire training set!\n",
    "        \n",
    "        dscore = crit.backward(bY)\n",
    "        model.backward(dscore)\n",
    "        \n",
    "        sgd(params, gradParams, eta, weight_decay = trainopt['lambda'])\n",
    "\n",
    "        start_idx = stop_idx % N\n",
    "        \n",
    "        if (iteration % trainopt['display_iter']) == 0:\n",
    "            #compute train and val error; multiply by 100 for readability (make it percentage points)\n",
    "            trainError = 100 * error_rate(X, Y, model)\n",
    "            valError = 100 * error_rate(Xval, Yval, model)\n",
    "            print('{:8} batch loss: {:.3f} train error: {:.3f} val error: {:.3f}'.format(iteration, loss, trainError, valError))\n",
    "            \n",
    "            # early stopping: save the best model snapshot so far (i.e., model with lowest val error)\n",
    "            if valError < minValError:\n",
    "                saved_model = deepcopy(model)\n",
    "                minValError = valError\n",
    "        \n",
    "    return saved_model, minValError, trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, hidden_size, output_size, activation_func = 'ReLU', dropout = 0):\n",
    "    \"\"\"\n",
    "    Build a 2-layer model:\n",
    "    input_size: the dimension of input data\n",
    "    hidden_size: the dimension of hidden vector, hidden_size == 0 means only one layer\n",
    "    output_size: the output size of final layer.\n",
    "    activation_func: ReLU, sigmoid (defined above), Tanh (you'd have to define), etc. \n",
    "    dropout: the dropout rate: if dropout == 0, this is equivalent to no dropout\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    if type(hidden_size) is int:\n",
    "        hidden_size = [hidden_size] # ensure it's a list\n",
    "    \n",
    "    prev_size=input_size\n",
    "    \n",
    "    # add hidden layer(s) as requested\n",
    "    if hidden_size[0] == 0: # no hidden layer\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        for l in range(len(hidden_size)):\n",
    "            # ADD CODE to add a fully connected layer \n",
    "            \n",
    "            prev_size=hidden_size[l]\n",
    "\n",
    "            # ADD CODE to add a Relu\n",
    "                 \n",
    "            if dropout > 0:\n",
    "                #ADD CODE to add Dropout \n",
    "            \n",
    "                \n",
    "                \n",
    "    # ADD CODE to add output layer  (which is a fully connected layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainopt = {\n",
    "    'eta': 1e-3,   # initial learning rate\n",
    "    'maxiter': 10000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 5000,  # display batch loss every display_iter updates\n",
    "    'batch_size': 128,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25,  # drop eta after every eta_frac*maxiter\n",
    "    'update': 'sgd'\n",
    "}\n",
    "NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "# we will maintain a record of models trained for different values of lambda\n",
    "# these will be indexed directly by lambda value itself\n",
    "trained_models = dict()\n",
    "\n",
    "# choose the set of hyperparameters to explore\n",
    "\n",
    "lambda_=0.0\n",
    "hidden_size_=[] # ADD CODE to specify hidden dim for each layer; \n",
    "trainopt['lambda'] = lambda_\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.1) \n",
    "crit = SoftMaxLoss()\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a Kaggle submission file using best_trained_model which you should set based on your experiments\n",
    "kaggleX = FMNIST_utils.load_data('kaggle')\n",
    "if kaggleX.max() > 1: kaggleX = kaggleX/255\n",
    "kaggleYhat = predict(kaggleX, trained_model).argmax(-1)\n",
    "save_submission('submission-fmnist.csv', kaggleYhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
