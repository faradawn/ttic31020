{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTIC 31020 Introduction to Statistical Machine Learning: Neural Networks\n",
    "---\n",
    "\n",
    "In this notebook you will perform classification on the Fashion MNIST dataset with neural networks. Your task is (mostly) to implement the forward and backward methods for different layers (forward methods compute a layer's output given its input, while backward methods compute gradients for its parameters and its input given the gradient of its output).\n",
    "\n",
    "After filling the missing code, try to achieve the best performance by changing the hyperparameters. Neural networks are typically more hyperparameter-sensitive than other methods you've seen in the past homeworks, so good hyperparameter tuning is crucial to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superclass of neural network \"modules\" (layers)\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Module is a super class. It could be a single layer, or a multilayer perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        z = f(a); a is the input, and h is the output.\n",
    "        \n",
    "        Inputs:\n",
    "        _input: a\n",
    "        \n",
    "        Returns:\n",
    "        output z\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Compute:\n",
    "        gradient w.r.t. _input\n",
    "        gradient w.r.t. trainable parameters\n",
    "        \n",
    "        Inputs (in lecture notation):\n",
    "        _input: a \n",
    "        _gradOutput: dL/dz\n",
    "        \n",
    "        Returns:\n",
    "        gradInput: dL/dz\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return the value of trainable parameters and its corresponding gradient (Used for grandient descent)\n",
    "        \n",
    "        Returns:\n",
    "        params, gradParams\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn the module into training mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn the module into evaluate mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class representing a sequence of modules (a layered network)\n",
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Sequential provides a way to plug layers together in a feed-forward manner.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        self.layers = [] # layers contain all the layers in order\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer) # Add another layer at the end\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.layers) # How many layers.\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        Feed forward through all the layers, and return the output of the last layer\n",
    "        \"\"\"\n",
    "        for i in range(self.size()):\n",
    "            # The output of (i-1)-th layer is the _input of i-th layer\n",
    "            _input = self.layers[i].forward(_input)\n",
    "        self._output = _input\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _gradOutput):\n",
    "        \"\"\"\n",
    "        Backpropagate through all the layers using chain rule.\n",
    "        \"\"\"\n",
    "        for i in reversed(range(self.size())):\n",
    "            # The (i-1)-th layer receives the error from the i-th layer\n",
    "            _gradOutput = self.layers[i].backward(_gradOutput)\n",
    "        return _gradOutput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return trainable parameters and its corresponding gradient in a list\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        gradParams = []\n",
    "        for m in self.layers:\n",
    "            p, g = m.parameters()\n",
    "            if p is not None:\n",
    "                for _p, _g in zip(p,g):\n",
    "                    params.append(_p)\n",
    "                    gradParams.append(_g)\n",
    "        return params, gradParams\n",
    "\n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into training mode\n",
    "        \"\"\"\n",
    "        Module.training(self)\n",
    "        for m in self.layers:\n",
    "            m.training()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into evaluate mode\n",
    "        \"\"\"\n",
    "        Module.evaluate(self)\n",
    "        for m in self.layers:\n",
    "            m.evaluate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(x):\n",
    "    print(\"print_shape: The shape is {}, {}\".format(np.shape(x)[0], np.shape(x)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer (parameters include a matrix of weights a vector of biases)\n",
    "    \"\"\"\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        Module.__init__(self)\n",
    "        \n",
    "        # ADD CODE here to initialize the weights and biases\n",
    "        # Use the Xavier scheme over here\n",
    "        sigma = math.sqrt(1/inputSize)\n",
    "        self.weight = np.random.normal(0, sigma, (outputSize, inputSize))\n",
    "        self.bias = np.random.normal(0, sigma, outputSize)\n",
    "        \n",
    "        self.gradWeight = np.ndarray((inputSize, outputSize))\n",
    "        self.gradBias = np.ndarray(outputSize)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = W * input + b\n",
    "        \"\"\"\n",
    "        self._input = _input     \n",
    "\n",
    "        # ADD CODE to compute the layer's output\n",
    "        # The shape of the input is 8 * 10\n",
    "        # The shape of the weight is 3 * 10\n",
    "        # The shape of the bias is (8,)\n",
    "        # print_shape(self.weight)\n",
    "        # print_shape(self._input)\n",
    "        # print(np.shape(self.bias))\n",
    "        self._output =  np.matmul(self._input, self.weight.T)\n",
    "            \n",
    "        # self.weight, self._input.T).T\n",
    "        # print_shape(self._output)\n",
    "        # print(\"The shape is {}\".format(np.shape(self.bias)[0]))\n",
    "        for i in range(len(_input)):\n",
    "            self._output[i] += self.bias\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradWeight = gradOutput * input\n",
    "        gradBias = gradOutput * vec(1)\n",
    "        gradInput =  Weight * gradOutput\n",
    "        Wight's shape is (outputSize, inputSize)\n",
    "        \"\"\"\n",
    "        self.gradWeight.fill(0)\n",
    "        self.gradBias.fill(0)\n",
    "\n",
    "        # print_shape(_gradOutput)\n",
    "        # print_shape(self._input)\n",
    "        # print_shape(self.gradWeight)\n",
    "        \n",
    "        # ADD CODE to compute the gradient for the layer's weight\n",
    "        #print_shape(self.gradWeight)\n",
    "        self.gradWeight += np.dot(self._input.T, _gradOutput)\n",
    "        #self.gradWeight += np.dot(_gradOutput.T, self._input)\n",
    "    \n",
    "        #print(\"the size of the _gradOutput is {}, {}\".format(np.shape(_gradOutput)[0], np.shape(_gradOutput)[1]))\n",
    "        # ADD CODE to compute the gradient for the layer's bias\n",
    "        # print(\"The shape is {}\".format(np.shape(self.gradBias)[0]))\n",
    "        # print(np.shape(np.matmul(_gradOutput.T, np.ones(len(_gradOutput)))))\n",
    "        #print_shape(self.gradBias)\n",
    "        self.gradBias += np.matmul(_gradOutput.T, np.ones(len(_gradOutput)))\n",
    "        # ADD CODE to compute the gradient for the layer's input\n",
    "        #print_shape(self._gradInput)\n",
    "        self._gradInput = np.dot(_gradOutput, self.weight)\n",
    "\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return weight and bias and their g\n",
    "        \"\"\"\n",
    "        return [self.weight, self.bias], [self.gradWeight, self.gradBias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    ReLU activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = max(0, input)\n",
    "        \"\"\"\n",
    "        self._input = _input\n",
    "        self._output = _input\n",
    "        # ADD CODE to compute the layer's output\n",
    "    \n",
    "        # The input has size (3,10)\n",
    "        # The output has size (3,10)\n",
    "        for i in range(len(_input)):\n",
    "            self._output[i] =  np.maximum(np.zeros(len(self._input[i])), self._input[i])\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradInput = gradOutput * mask\n",
    "        mask = _input > 0\n",
    "        \"\"\"\n",
    "        # ADD CODE to compute the gradient for the layer's input\n",
    "        # If input is 0, grad_Input is also 1\n",
    "        # If input is not zero, grad_Input is _gradOutput\n",
    "        self._gradInput = _gradOutput * (self._input > 0)\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parametersm, return None\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    A dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p = 0.5):\n",
    "        Module.__init__(self)\n",
    "        self.p = p #self.p is the drop rate, if self.p is 0, then it's a identity layer\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        self._output = _input\n",
    "        if self.p > 0:\n",
    "            if self.train:\n",
    "                # Randomize a mask from bernoulli distrubition\n",
    "                self.mask = np.random.binomial(1, 1 - self.p, _input.shape).astype('float64')\n",
    "                # Scale the mask\n",
    "                self.mask /= 1 - self.p\n",
    "                self._output *= self.mask\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _gradOutput):\n",
    "        self._gradInput = _gradOutput\n",
    "        if self.train:\n",
    "            if self.p > 0:\n",
    "                self._gradInput = self.mask* self._gradInput \n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parameters.\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxLoss(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def forward(self, _input, _label):\n",
    "        \"\"\"\n",
    "        Softmax and cross entropy loss layer. Should return a scalar, since it's a\n",
    "        loss. (It's almost identical to what we had in Pset 2)\n",
    "\n",
    "        Inputs:\n",
    "        _input: N x C\n",
    "        _labels: N x C, one-hot\n",
    "\n",
    "        Returns: loss (scalar)\n",
    "        \"\"\"\n",
    "        self._input = _input - _input.max(1)[:, np.newaxis] # doing the adjustment for softmax\n",
    "        # calculate log probability\n",
    "        self._logprob = self._input - np.log(np.exp(self._input).sum(1)[:, np.newaxis])\n",
    "        \n",
    "        self._output = np.mean(np.sum(-self._logprob * _label, 1))\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _label):\n",
    "        self._gradInput = (np.exp(self._logprob) - _label) / len(self._input) # ADD CODE to compute the gradient for the layer's input\n",
    "        return self._gradInput\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4270368746847275\n",
      "6.245556800592332e-09\n"
     ]
    }
   ],
   "source": [
    "# Test softmaxloss, the relative error should be small enough\n",
    "def test_sm():\n",
    "    crit = SoftMaxLoss()\n",
    "    gt = np.zeros((3, 10))\n",
    "    gt[np.arange(3), np.array([1,2,3])] = 1\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(x, gt)\n",
    "\n",
    "    print(crit.forward(x, gt))\n",
    "    gradInput = crit.backward(gt)\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "    \n",
    "test_sm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing FullyConnected\n",
      "4.919703731261059e-09\n",
      "testing ReLU\n",
      "7.360228254783035e-10\n",
      "testing Dropout\n",
      "5.962448038856628e-10\n"
     ]
    }
   ],
   "source": [
    "# Test modules, all the relative errors should be small enough (on the order of 1e-6 or smaller)\n",
    "def test_module(model):\n",
    "    model.evaluate()\n",
    "\n",
    "    crit = TestCriterion()\n",
    "    gt = np.random.random((3,10))\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(model.forward(x), gt)\n",
    "\n",
    "    test_f(x)\n",
    "    gradInput = model.backward(crit.backward(gt))\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "\n",
    "# Test fully connected\n",
    "model = FullyConnected(10, 8)\n",
    "print('testing FullyConnected')\n",
    "test_module(model)\n",
    "\n",
    "# Test ReLU\n",
    "model = ReLU()\n",
    "print('testing ReLU')\n",
    "test_module(model)\n",
    "\n",
    "# Test Dropout\n",
    "model = Dropout()\n",
    "print('testing Dropout')\n",
    "test_module(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#ADD CODE to add layers using the add attribute of sequential \n",
    "#to construct 2-layer Neural Network with hidden size 10\n",
    "\n",
    "layer_one = FullyConnected(10, 10)\n",
    "model.add(layer_one)\n",
    "model.add(ReLU())\n",
    "layer_two = FullyConnected(10, 10)\n",
    "model.add(layer_two)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing 2-layer model\n",
      "7.855736459777633e-08\n"
     ]
    }
   ],
   "source": [
    "# Test your neural network\n",
    "print('testing 2-layer model')\n",
    "test_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x, dx, lr, weight_decay = 0):\n",
    "    #ADD CODE to perform one gradient descent step\n",
    "    for i in range(len(x)):\n",
    "        if ((i % 2) == 0):\n",
    "            x[i] -= lr * (dx[i].T + 2 * weight_decay * x[i]) \n",
    "        else:\n",
    "            x[i] -= lr * (dx[i].T)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.154806692439288\n",
      "0.3620601236660053\n",
      "0.24197890475411726\n",
      "0.22145627006487606\n",
      "0.1556062346147803\n",
      "0.17418336554494032\n",
      "0.15036445411096092\n",
      "0.15084613175289693\n",
      "0.1545282318142583\n",
      "0.09557576715686501\n",
      "0.0966292867312172\n"
     ]
    }
   ],
   "source": [
    "# Test gradient descent, the loss should be lower and lower\n",
    "trainX = np.random.random((10,10))\n",
    "\n",
    "crit = TestCriterion()\n",
    "\n",
    "params, gradParams = model.parameters()\n",
    "\n",
    "it = 0\n",
    "state = None\n",
    "while True:\n",
    "    output = model.forward(trainX)\n",
    "    loss = crit.forward(output, None)\n",
    "    if it % 100 == 0:\n",
    "        print(loss)\n",
    "    doutput = crit.backward(None)\n",
    "    model.backward(doutput)\n",
    "    # print(np.shape(params))\n",
    "    # print(np.shape(gradParams))\n",
    "\n",
    "    sgd(params, gradParams, 0.01)\n",
    "    if it > 1000:\n",
    "        break\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start to work on Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load large trainset.\n",
      "(50000, 784)\n",
      "(50000, 10)\n",
      "Load valset.\n",
      "(5000, 784)\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "import FMNIST_utils\n",
    "\n",
    "# We only consider large set this time\n",
    "print(\"Load large trainset.\")\n",
    "Xlarge,Ylarge = FMNIST_utils.load_data(\"Tr\")\n",
    "print(Xlarge.shape)\n",
    "print(Ylarge.shape)\n",
    "if Xlarge.max() > 1: Xlarge = Xlarge/255\n",
    "\n",
    "print(\"Load valset.\")\n",
    "Xval,Yval = FMNIST_utils.load_data(\"Vl\")\n",
    "print(Xval.shape)\n",
    "print(Yval.shape)\n",
    "if Xval.max() > 1: Xval = Xval/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"\n",
    "    Evaluate the soft predictions of the model.\n",
    "    Input:\n",
    "    X : N x d array (no unit terms)\n",
    "    model : a multi-layer perceptron\n",
    "    Output:\n",
    "    yhat : N x C array\n",
    "        yhat[n][:] contains the score over C classes for X[n][:]\n",
    "    \"\"\"\n",
    "    return model.forward(X)\n",
    "\n",
    "def error_rate(X, Y, model):\n",
    "    \"\"\"\n",
    "    Compute error rate (between 0 and 1) for the model\n",
    "    \"\"\"\n",
    "    model.evaluate()\n",
    "    res = 1 - (model.forward(X).argmax(-1) == Y.argmax(-1)).mean()\n",
    "    model.training()\n",
    "    return res\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def runTrainVal(X,Y,model,Xval,Yval,trainopt):\n",
    "    \"\"\"\n",
    "    Run the train + evaluation on a given train/val partition\n",
    "    trainopt: various (hyper)parameters of the training procedure\n",
    "    During training, choose the model with the lowest validation error. (early stopping)\n",
    "    Assumes (global) variable crit containing the loss (training \"criterion\" to be minimized)\n",
    "    \"\"\"\n",
    "    \n",
    "    params, gradParams = model.parameters()\n",
    "    \n",
    "    eta = trainopt['eta']\n",
    "    \n",
    "    N = X.shape[0] # number of data points in X\n",
    "    \n",
    "    # Save the model with lowest validation error\n",
    "    minValError = np.inf\n",
    "    saved_model = None # Save the best model accoring to validation error\n",
    "    \n",
    "    shuffled_idx = np.random.permutation(N)\n",
    "    start_idx = 0\n",
    "    for iteration in range(trainopt['maxiter']):\n",
    "        if iteration % int(trainopt['eta_frac'] * trainopt['maxiter']) == 0:\n",
    "            eta *= trainopt['etadrop']\n",
    "        # form the next mini-batch\n",
    "        stop_idx = min(start_idx + trainopt['batch_size'], N)\n",
    "        batch_idx = range(N)[int(start_idx):int(stop_idx)]\n",
    "        \n",
    "        s_idx = shuffled_idx[batch_idx]\n",
    "        \n",
    "        bX = X[s_idx,:]\n",
    "        bY = Y[s_idx,:]\n",
    "\n",
    "        score = model.forward(bX)\n",
    "        loss = crit.forward(score, bY)\n",
    "        # note: this computes loss on the *batch* only, not on the entire training set!\n",
    "        \n",
    "        dscore = crit.backward(bY)\n",
    "        model.backward(dscore)\n",
    "        \n",
    "        sgd(params, gradParams, eta, weight_decay = trainopt['lambda'])\n",
    "\n",
    "        start_idx = stop_idx % N\n",
    "        \n",
    "        if (iteration % trainopt['display_iter']) == 0:\n",
    "            #compute train and val error; multiply by 100 for readability (make it percentage points)\n",
    "            trainError = 100 * error_rate(X, Y, model)\n",
    "            valError = 100 * error_rate(Xval, Yval, model)\n",
    "            print('{:8} batch loss: {:.3f} train error: {:.3f} val error: {:.3f}'.format(iteration, loss, trainError, valError))\n",
    "            \n",
    "            # early stopping: save the best model snapshot so far (i.e., model with lowest val error)\n",
    "            if valError < minValError:\n",
    "                saved_model = deepcopy(model)\n",
    "                minValError = valError\n",
    "        \n",
    "    return saved_model, minValError, trainError, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, hidden_size, output_size, activation_func = 'ReLU', dropout = 0):\n",
    "    \"\"\"\n",
    "    Build a 2-layer model:\n",
    "    input_size: the dimension of input data\n",
    "    hidden_size: the dimension of hidden vector, hidden_size == 0 means only one layer\n",
    "    output_size: the output size of final layer.\n",
    "    activation_func: ReLU, sigmoid (defined above), Tanh (you'd have to define), etc. \n",
    "    dropout: the dropout rate: if dropout == 0, this is equivalent to no dropout\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    if type(hidden_size) is int:\n",
    "        hidden_size = [hidden_size] # ensure it's a list\n",
    "    \n",
    "    prev_size=input_size\n",
    "    \n",
    "    # add hidden layer(s) as requested\n",
    "    if hidden_size[0] == 0: # no hidden layer\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        for l in range(len(hidden_size)):\n",
    "            # ADD CODE to add a fully connected layer \n",
    "            each_layer = FullyConnected(prev_size, hidden_size[l])\n",
    "            model.add(each_layer)\n",
    "            \n",
    "            prev_size=hidden_size[l]\n",
    "            # ADD CODE to add a Relu\n",
    "            relu_layer = ReLU()\n",
    "            model.add(relu_layer)\n",
    "            \n",
    "            if dropout > 0:\n",
    "                #ADD CODE to add Dropout \n",
    "                dropout_layer = Dropout(dropout)\n",
    "                model.add(dropout_layer)\n",
    "                     \n",
    "                \n",
    "    # ADD CODE to add output layer  (which is a fully connected layer)\n",
    "    output_layer = FullyConnected(hidden_size[-1], output_size)\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.349 train error: 91.022 val error: 90.740\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-488-e8308145a7a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftMaxLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# -- model trained on large train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalErr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainErr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunTrainVal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlarge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYlarge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtrained_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_err\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalErr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_err\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainErr\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train set model [ h = '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-486-785468b5b765>\u001b[0m in \u001b[0;36mrunTrainVal\u001b[0;34m(X, Y, model, Xval, Yval, trainopt)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mstart_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-477-8723c0703907>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(x, dx, lr, weight_decay)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainopt = {\n",
    "    'eta': 1e-3,   # initial learning rate\n",
    "    'maxiter': 10000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 1000,  # display batch loss every display_iter updates, originally 5000\n",
    "    'batch_size': 128,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25,  # drop eta after every eta_frac*maxiter\n",
    "    'update': 'sgd'\n",
    "}\n",
    "NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "# we will maintain a record of models trained for different values of lambda\n",
    "# these will be indexed directly by lambda value itself\n",
    "trained_models = dict()\n",
    "\n",
    "# choose the set of hyperparameters to explore\n",
    "\n",
    "lambda_=0.0\n",
    "\n",
    "hidden_size_=[2000] # ADD CODE to specify hidden dim for each layer; \n",
    "trainopt['lambda'] = lambda_\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.1) \n",
    "crit = SoftMaxLoss()\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.347 train error: 92.692 val error: 92.440\n",
      "    1000 batch loss: 2.028 train error: 60.988 val error: 61.560\n",
      "    2000 batch loss: 1.728 train error: 49.572 val error: 50.500\n",
      "    3000 batch loss: 1.561 train error: 44.352 val error: 44.620\n",
      "    4000 batch loss: 1.461 train error: 38.208 val error: 38.520\n",
      "    5000 batch loss: 1.391 train error: 36.428 val error: 36.640\n",
      "    6000 batch loss: 1.457 train error: 36.142 val error: 36.180\n",
      "    7000 batch loss: 1.501 train error: 35.856 val error: 35.580\n",
      "    8000 batch loss: 1.487 train error: 35.784 val error: 35.540\n",
      "    9000 batch loss: 1.349 train error: 35.686 val error: 35.420\n",
      "train set model [ h = 20  ], lambda= 0.0100 ] --> train error: 35.69, val error: 35.42\n",
      "       0 batch loss: 2.423 train error: 89.880 val error: 89.640\n",
      "    1000 batch loss: 1.995 train error: 73.800 val error: 75.240\n",
      "    2000 batch loss: 1.905 train error: 54.552 val error: 54.900\n",
      "    3000 batch loss: 1.627 train error: 42.248 val error: 42.100\n",
      "    4000 batch loss: 1.529 train error: 39.274 val error: 39.340\n",
      "    5000 batch loss: 1.557 train error: 36.696 val error: 37.220\n",
      "    6000 batch loss: 1.467 train error: 34.708 val error: 35.420\n",
      "    7000 batch loss: 1.495 train error: 33.018 val error: 33.520\n",
      "    8000 batch loss: 1.396 train error: 32.458 val error: 33.120\n",
      "    9000 batch loss: 1.422 train error: 32.238 val error: 32.780\n",
      "train set model [ h = 20  ], lambda= 0.0300 ] --> train error: 32.24, val error: 32.78\n",
      "       0 batch loss: 2.391 train error: 89.964 val error: 89.600\n",
      "    1000 batch loss: 1.990 train error: 70.452 val error: 71.700\n",
      "    2000 batch loss: 1.815 train error: 57.736 val error: 58.320\n",
      "    3000 batch loss: 1.694 train error: 44.380 val error: 44.320\n",
      "    4000 batch loss: 1.645 train error: 40.478 val error: 40.840\n",
      "    5000 batch loss: 1.571 train error: 38.484 val error: 38.860\n",
      "    6000 batch loss: 1.403 train error: 37.972 val error: 37.900\n",
      "    7000 batch loss: 1.421 train error: 37.324 val error: 37.240\n",
      "    8000 batch loss: 1.424 train error: 36.332 val error: 36.080\n",
      "    9000 batch loss: 1.471 train error: 35.610 val error: 35.300\n",
      "train set model [ h = 20  ], lambda= 0.0500 ] --> train error: 35.61, val error: 35.30\n",
      "       0 batch loss: 2.370 train error: 89.334 val error: 89.880\n",
      "    1000 batch loss: 2.062 train error: 57.612 val error: 58.640\n",
      "    2000 batch loss: 1.964 train error: 50.122 val error: 51.180\n",
      "    3000 batch loss: 1.820 train error: 43.318 val error: 44.540\n",
      "    4000 batch loss: 1.755 train error: 39.884 val error: 40.460\n",
      "    5000 batch loss: 1.728 train error: 37.678 val error: 38.120\n",
      "    6000 batch loss: 1.710 train error: 36.928 val error: 37.500\n",
      "    7000 batch loss: 1.757 train error: 36.286 val error: 36.860\n",
      "    8000 batch loss: 1.672 train error: 35.994 val error: 36.260\n",
      "    9000 batch loss: 1.627 train error: 35.678 val error: 35.940\n",
      "train set model [ h = 20  ], lambda= 0.0100 ] --> train error: 35.68, val error: 35.94\n",
      "       0 batch loss: 2.456 train error: 87.354 val error: 87.860\n",
      "    1000 batch loss: 2.106 train error: 66.266 val error: 65.680\n",
      "    2000 batch loss: 2.013 train error: 53.978 val error: 53.560\n",
      "    3000 batch loss: 1.874 train error: 49.288 val error: 49.080\n",
      "    4000 batch loss: 1.899 train error: 46.854 val error: 46.660\n",
      "    5000 batch loss: 1.759 train error: 45.134 val error: 45.100\n",
      "    6000 batch loss: 1.783 train error: 44.204 val error: 44.120\n",
      "    7000 batch loss: 1.741 train error: 43.086 val error: 43.300\n",
      "    8000 batch loss: 1.820 train error: 42.400 val error: 42.500\n",
      "    9000 batch loss: 1.807 train error: 41.844 val error: 41.940\n",
      "train set model [ h = 20  ], lambda= 0.0300 ] --> train error: 41.84, val error: 41.94\n",
      "       0 batch loss: 2.450 train error: 89.078 val error: 89.300\n",
      "    1000 batch loss: 2.053 train error: 58.824 val error: 59.220\n",
      "    2000 batch loss: 1.940 train error: 49.338 val error: 50.260\n",
      "    3000 batch loss: 1.892 train error: 44.528 val error: 44.880\n",
      "    4000 batch loss: 1.658 train error: 41.524 val error: 41.740\n",
      "    5000 batch loss: 1.759 train error: 38.934 val error: 39.260\n",
      "    6000 batch loss: 1.736 train error: 38.054 val error: 38.480\n",
      "    7000 batch loss: 1.764 train error: 37.156 val error: 37.480\n",
      "    8000 batch loss: 1.703 train error: 36.496 val error: 37.020\n",
      "    9000 batch loss: 1.709 train error: 36.118 val error: 36.600\n",
      "train set model [ h = 20  ], lambda= 0.0500 ] --> train error: 36.12, val error: 36.60\n",
      "       0 batch loss: 2.430 train error: 92.792 val error: 92.720\n",
      "    1000 batch loss: 1.792 train error: 39.596 val error: 39.820\n",
      "    2000 batch loss: 1.435 train error: 34.362 val error: 35.140\n",
      "    3000 batch loss: 1.341 train error: 33.064 val error: 33.440\n",
      "    4000 batch loss: 1.177 train error: 32.594 val error: 32.740\n",
      "    5000 batch loss: 1.189 train error: 32.056 val error: 32.140\n",
      "    6000 batch loss: 1.219 train error: 31.846 val error: 31.980\n",
      "    7000 batch loss: 1.164 train error: 31.600 val error: 31.560\n",
      "    8000 batch loss: 1.100 train error: 31.400 val error: 31.440\n",
      "    9000 batch loss: 1.189 train error: 31.278 val error: 31.260\n",
      "train set model [ h = 200  ], lambda= 0.0100 ] --> train error: 31.28, val error: 31.26\n",
      "       0 batch loss: 2.572 train error: 89.850 val error: 89.460\n",
      "    1000 batch loss: 1.857 train error: 44.232 val error: 45.280\n",
      "    2000 batch loss: 1.520 train error: 35.432 val error: 35.740\n",
      "    3000 batch loss: 1.400 train error: 34.266 val error: 34.820\n",
      "    4000 batch loss: 1.245 train error: 33.844 val error: 34.260\n",
      "    5000 batch loss: 1.253 train error: 33.440 val error: 33.820\n",
      "    6000 batch loss: 1.222 train error: 33.236 val error: 33.460\n",
      "    7000 batch loss: 1.176 train error: 33.034 val error: 33.240\n",
      "    8000 batch loss: 1.144 train error: 32.920 val error: 33.120\n",
      "    9000 batch loss: 1.174 train error: 32.814 val error: 33.040\n",
      "train set model [ h = 200  ], lambda= 0.0300 ] --> train error: 32.81, val error: 33.04\n",
      "       0 batch loss: 2.390 train error: 95.046 val error: 95.160\n",
      "    1000 batch loss: 1.824 train error: 40.942 val error: 42.340\n",
      "    2000 batch loss: 1.481 train error: 32.906 val error: 33.780\n",
      "    3000 batch loss: 1.362 train error: 32.296 val error: 32.820\n",
      "    4000 batch loss: 1.310 train error: 32.118 val error: 32.340\n",
      "    5000 batch loss: 1.332 train error: 31.924 val error: 31.940\n",
      "    6000 batch loss: 1.294 train error: 31.778 val error: 31.960\n",
      "    7000 batch loss: 1.314 train error: 31.654 val error: 31.820\n",
      "    8000 batch loss: 1.156 train error: 31.584 val error: 31.820\n",
      "    9000 batch loss: 1.184 train error: 31.510 val error: 31.640\n",
      "train set model [ h = 200  ], lambda= 0.0500 ] --> train error: 31.51, val error: 31.64\n",
      "       0 batch loss: 2.406 train error: 86.740 val error: 86.340\n",
      "    1000 batch loss: 1.875 train error: 45.898 val error: 46.660\n",
      "    2000 batch loss: 1.631 train error: 36.424 val error: 36.720\n",
      "    3000 batch loss: 1.402 train error: 34.618 val error: 34.760\n",
      "    4000 batch loss: 1.353 train error: 33.950 val error: 34.020\n",
      "    5000 batch loss: 1.375 train error: 33.302 val error: 33.420\n",
      "    6000 batch loss: 1.232 train error: 33.048 val error: 33.020\n",
      "    7000 batch loss: 1.207 train error: 32.780 val error: 32.740\n",
      "    8000 batch loss: 1.291 train error: 32.610 val error: 32.680\n",
      "    9000 batch loss: 1.268 train error: 32.492 val error: 32.560\n",
      "train set model [ h = 200  ], lambda= 0.0100 ] --> train error: 32.49, val error: 32.56\n",
      "       0 batch loss: 2.371 train error: 87.572 val error: 87.800\n",
      "    1000 batch loss: 1.832 train error: 42.564 val error: 43.120\n",
      "    2000 batch loss: 1.625 train error: 35.548 val error: 35.960\n",
      "    3000 batch loss: 1.500 train error: 34.260 val error: 34.560\n",
      "    4000 batch loss: 1.467 train error: 33.788 val error: 33.800\n",
      "    5000 batch loss: 1.329 train error: 33.402 val error: 33.320\n",
      "    6000 batch loss: 1.195 train error: 33.148 val error: 33.080\n",
      "    7000 batch loss: 1.130 train error: 32.958 val error: 32.960\n",
      "    8000 batch loss: 1.305 train error: 32.776 val error: 32.660\n",
      "    9000 batch loss: 1.225 train error: 32.732 val error: 32.680\n",
      "train set model [ h = 200  ], lambda= 0.0300 ] --> train error: 32.73, val error: 32.66\n",
      "       0 batch loss: 2.365 train error: 88.908 val error: 89.280\n",
      "    1000 batch loss: 1.990 train error: 41.256 val error: 41.440\n",
      "    2000 batch loss: 1.654 train error: 35.836 val error: 36.120\n",
      "    3000 batch loss: 1.439 train error: 34.658 val error: 35.420\n",
      "    4000 batch loss: 1.454 train error: 34.156 val error: 34.840\n",
      "    5000 batch loss: 1.422 train error: 33.910 val error: 34.480\n",
      "    6000 batch loss: 1.342 train error: 33.772 val error: 34.340\n",
      "    7000 batch loss: 1.382 train error: 33.664 val error: 34.220\n",
      "    8000 batch loss: 1.443 train error: 33.530 val error: 33.980\n",
      "    9000 batch loss: 1.266 train error: 33.468 val error: 33.960\n",
      "train set model [ h = 200  ], lambda= 0.0500 ] --> train error: 33.47, val error: 33.96\n",
      "       0 batch loss: 2.363 train error: 92.910 val error: 92.320\n",
      "    1000 batch loss: 1.582 train error: 37.522 val error: 38.560\n",
      "    2000 batch loss: 1.258 train error: 32.580 val error: 32.300\n",
      "    3000 batch loss: 1.181 train error: 31.492 val error: 31.340\n",
      "    4000 batch loss: 1.081 train error: 30.576 val error: 30.640\n",
      "    5000 batch loss: 1.032 train error: 29.616 val error: 29.520\n",
      "    6000 batch loss: 0.982 train error: 29.230 val error: 29.040\n",
      "    7000 batch loss: 0.984 train error: 28.772 val error: 28.520\n",
      "    8000 batch loss: 0.959 train error: 28.480 val error: 28.260\n",
      "    9000 batch loss: 0.902 train error: 28.266 val error: 27.960\n",
      "train set model [ h = 1000  ], lambda= 0.0100 ] --> train error: 28.27, val error: 27.96\n",
      "       0 batch loss: 2.362 train error: 86.460 val error: 86.540\n",
      "    1000 batch loss: 1.657 train error: 37.350 val error: 37.780\n",
      "    2000 batch loss: 1.234 train error: 32.504 val error: 32.980\n",
      "    3000 batch loss: 1.043 train error: 30.966 val error: 31.060\n",
      "    4000 batch loss: 1.119 train error: 30.204 val error: 30.080\n",
      "    5000 batch loss: 0.915 train error: 29.424 val error: 29.160\n",
      "    6000 batch loss: 1.046 train error: 29.124 val error: 28.700\n",
      "    7000 batch loss: 1.025 train error: 28.758 val error: 28.440\n",
      "    8000 batch loss: 0.956 train error: 28.518 val error: 28.140\n",
      "    9000 batch loss: 0.910 train error: 28.356 val error: 27.960\n",
      "train set model [ h = 1000  ], lambda= 0.0300 ] --> train error: 28.36, val error: 27.96\n",
      "       0 batch loss: 2.373 train error: 86.460 val error: 86.180\n",
      "    1000 batch loss: 1.595 train error: 39.106 val error: 39.440\n",
      "    2000 batch loss: 1.415 train error: 34.076 val error: 34.400\n",
      "    3000 batch loss: 1.227 train error: 32.946 val error: 33.060\n",
      "    4000 batch loss: 1.201 train error: 32.188 val error: 32.360\n",
      "    5000 batch loss: 1.141 train error: 31.662 val error: 31.780\n",
      "    6000 batch loss: 1.042 train error: 31.322 val error: 31.220\n",
      "    7000 batch loss: 1.051 train error: 31.082 val error: 31.040\n",
      "    8000 batch loss: 0.973 train error: 30.874 val error: 30.800\n",
      "    9000 batch loss: 1.009 train error: 30.784 val error: 30.660\n",
      "train set model [ h = 1000  ], lambda= 0.0500 ] --> train error: 30.78, val error: 30.66\n",
      "       0 batch loss: 2.383 train error: 92.590 val error: 92.240\n",
      "    1000 batch loss: 1.509 train error: 36.284 val error: 36.540\n",
      "    2000 batch loss: 1.356 train error: 33.072 val error: 33.000\n",
      "    3000 batch loss: 1.162 train error: 31.944 val error: 31.720\n",
      "    4000 batch loss: 1.047 train error: 30.950 val error: 30.480\n",
      "    5000 batch loss: 1.089 train error: 30.150 val error: 29.560\n",
      "    6000 batch loss: 1.080 train error: 29.710 val error: 29.220\n",
      "    7000 batch loss: 0.984 train error: 29.216 val error: 28.740\n",
      "    8000 batch loss: 1.077 train error: 28.880 val error: 28.560\n",
      "    9000 batch loss: 1.087 train error: 28.664 val error: 28.300\n",
      "train set model [ h = 1000  ], lambda= 0.0100 ] --> train error: 28.66, val error: 28.30\n",
      "       0 batch loss: 2.338 train error: 90.682 val error: 91.020\n",
      "    1000 batch loss: 1.631 train error: 35.732 val error: 35.800\n",
      "    2000 batch loss: 1.278 train error: 33.562 val error: 33.700\n",
      "    3000 batch loss: 1.227 train error: 32.250 val error: 32.380\n",
      "    4000 batch loss: 1.104 train error: 31.536 val error: 31.460\n",
      "    5000 batch loss: 1.161 train error: 30.692 val error: 30.460\n",
      "    6000 batch loss: 1.031 train error: 30.390 val error: 30.080\n",
      "    7000 batch loss: 1.042 train error: 29.968 val error: 29.600\n",
      "    8000 batch loss: 1.070 train error: 29.678 val error: 29.340\n",
      "    9000 batch loss: 0.964 train error: 29.492 val error: 28.980\n",
      "train set model [ h = 1000  ], lambda= 0.0300 ] --> train error: 29.49, val error: 28.98\n",
      "       0 batch loss: 2.300 train error: 89.014 val error: 88.980\n",
      "    1000 batch loss: 1.705 train error: 36.912 val error: 37.280\n",
      "    2000 batch loss: 1.354 train error: 33.548 val error: 34.260\n",
      "    3000 batch loss: 1.178 train error: 32.572 val error: 32.900\n",
      "    4000 batch loss: 1.242 train error: 32.090 val error: 32.240\n",
      "    5000 batch loss: 1.213 train error: 31.492 val error: 31.640\n",
      "    6000 batch loss: 1.059 train error: 31.234 val error: 31.220\n",
      "    7000 batch loss: 1.191 train error: 30.908 val error: 31.040\n",
      "    8000 batch loss: 1.083 train error: 30.722 val error: 30.940\n",
      "    9000 batch loss: 1.117 train error: 30.624 val error: 30.800\n",
      "train set model [ h = 1000  ], lambda= 0.0500 ] --> train error: 30.62, val error: 30.80\n",
      "best Loss is0.9100991775069014, best lambda is 0, best dropout rate is 1000, best hidden size is0.1\n"
     ]
    }
   ],
   "source": [
    "h_lst = [20, 200, 1000]\n",
    "dropout_lst = [0.1, 0.5]\n",
    "lambda_lst = [0.01, 0.03, 0.05]\n",
    "best_loss = None\n",
    "best_h = 0\n",
    "best_dropout_rate = 0\n",
    "best_lambda = 0\n",
    "\n",
    "\n",
    "for h in h_lst:\n",
    "    for d in dropout_lst:\n",
    "        for l in lambda_lst:\n",
    "            trainopt = {\n",
    "                'eta': 1e-3,   # initial learning rate\n",
    "                'maxiter': 10000,   # max number of iterations (updates) of SGD\n",
    "                'display_iter': 1000,  # display batch loss every display_iter updates, originally 5000\n",
    "                'batch_size': 128,  \n",
    "                'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "                'eta_frac': .25,  # drop eta after every eta_frac*maxiter\n",
    "                'update': 'sgd'\n",
    "            }\n",
    "            NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "            # we will maintain a record of models trained for different values of lambda\n",
    "            # these will be indexed directly by lambda value itself\n",
    "            trained_models = dict()\n",
    "\n",
    "            # choose the set of hyperparameters to explore\n",
    "\n",
    "            lambda_= l\n",
    "            hidden_size_=[h] # ADD CODE to specify hidden dim for each layer; \n",
    "            trainopt['lambda'] = lambda_\n",
    "            model = build_model(NFEATURES, hidden_size_, 10, dropout = d) \n",
    "            crit = SoftMaxLoss()\n",
    "            # -- model trained on large train set\n",
    "            trained_model,valErr,trainErr, loss = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "            trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr, 'loss': loss }\n",
    "            print('train set model [ h = ',end='')\n",
    "            for l in range(len(hidden_size_)):\n",
    "                print('%d '%hidden_size_[l],end='')\n",
    "            print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "            if best_loss == None:\n",
    "                best_loss = loss\n",
    "                best_lambda = l\n",
    "                best_h = h\n",
    "                best_dropout_rate = d\n",
    "            else:\n",
    "                if (best_loss > loss):\n",
    "                    best_loss = loss\n",
    "                    best_lambda = l\n",
    "                    best_h = h\n",
    "                    best_dropout_rate = d\n",
    "print(\"best Loss is{}, best lambda is {}, best dropout rate is {}, best hidden size is{}\".format(best_loss, best_lambda, best_h, best_dropout_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.344 train error: 86.194 val error: 85.840\n",
      "    1000 batch loss: 1.942 train error: 63.408 val error: 63.420\n",
      "    2000 batch loss: 1.770 train error: 49.844 val error: 50.100\n",
      "    3000 batch loss: 1.599 train error: 36.684 val error: 36.840\n",
      "    4000 batch loss: 1.438 train error: 33.104 val error: 33.220\n",
      "    5000 batch loss: 1.327 train error: 31.718 val error: 31.760\n",
      "    6000 batch loss: 1.344 train error: 31.346 val error: 31.340\n",
      "    7000 batch loss: 1.304 train error: 30.764 val error: 30.780\n",
      "    8000 batch loss: 1.238 train error: 30.356 val error: 30.300\n",
      "    9000 batch loss: 1.252 train error: 29.994 val error: 29.900\n",
      "train set model [ h = 20  ], lambda= 0.0300 ] --> train error: 29.99, val error: 29.90\n",
      "       0 batch loss: 2.406 train error: 89.922 val error: 89.120\n",
      "    1000 batch loss: 1.773 train error: 43.752 val error: 44.300\n",
      "    2000 batch loss: 1.482 train error: 34.894 val error: 34.960\n",
      "    3000 batch loss: 1.221 train error: 32.546 val error: 32.640\n",
      "    4000 batch loss: 1.144 train error: 31.014 val error: 30.780\n",
      "    5000 batch loss: 1.088 train error: 29.990 val error: 29.800\n",
      "    6000 batch loss: 1.022 train error: 29.486 val error: 29.140\n",
      "    7000 batch loss: 0.889 train error: 28.880 val error: 28.620\n",
      "    8000 batch loss: 0.890 train error: 28.476 val error: 28.300\n",
      "    9000 batch loss: 0.995 train error: 27.932 val error: 27.760\n",
      "train set model [ h = 200  ], lambda= 0.0000 ] --> train error: 27.93, val error: 27.76\n",
      "       0 batch loss: 2.361 train error: 89.938 val error: 89.600\n",
      "    1000 batch loss: 1.579 train error: 36.266 val error: 36.060\n",
      "    2000 batch loss: 1.235 train error: 33.424 val error: 32.940\n",
      "    3000 batch loss: 0.970 train error: 31.076 val error: 30.600\n",
      "    4000 batch loss: 0.901 train error: 29.092 val error: 28.440\n",
      "    5000 batch loss: 0.870 train error: 27.292 val error: 26.640\n",
      "    6000 batch loss: 0.809 train error: 26.736 val error: 26.340\n",
      "    7000 batch loss: 0.862 train error: 26.132 val error: 25.420\n",
      "    8000 batch loss: 0.879 train error: 25.716 val error: 25.160\n",
      "    9000 batch loss: 0.810 train error: 25.154 val error: 24.680\n",
      "train set model [ h = 1000  ], lambda= 0.0000 ] --> train error: 25.15, val error: 24.68\n",
      "       0 batch loss: 2.350 train error: 91.296 val error: 92.080\n",
      "    1000 batch loss: 1.428 train error: 33.614 val error: 33.840\n",
      "    2000 batch loss: 1.189 train error: 31.708 val error: 31.400\n",
      "    3000 batch loss: 0.973 train error: 28.780 val error: 28.520\n",
      "    4000 batch loss: 0.778 train error: 26.666 val error: 26.420\n",
      "    5000 batch loss: 0.828 train error: 25.186 val error: 24.860\n",
      "    6000 batch loss: 0.752 train error: 24.614 val error: 24.320\n",
      "    7000 batch loss: 0.801 train error: 24.060 val error: 23.620\n",
      "    8000 batch loss: 0.731 train error: 23.582 val error: 23.160\n",
      "    9000 batch loss: 0.772 train error: 23.142 val error: 23.020\n",
      "train set model [ h = 2000  ], lambda= 0.0000 ] --> train error: 23.14, val error: 23.02\n",
      "       0 batch loss: 2.401 train error: 87.834 val error: 87.360\n",
      "    1000 batch loss: 1.257 train error: 32.734 val error: 32.460\n",
      "    2000 batch loss: 0.907 train error: 29.110 val error: 28.740\n",
      "    3000 batch loss: 0.875 train error: 26.282 val error: 26.000\n",
      "    4000 batch loss: 0.834 train error: 24.490 val error: 23.900\n",
      "    5000 batch loss: 0.793 train error: 23.240 val error: 22.940\n",
      "    6000 batch loss: 0.850 train error: 22.598 val error: 22.480\n",
      "    7000 batch loss: 0.786 train error: 22.228 val error: 22.140\n",
      "    8000 batch loss: 0.733 train error: 21.778 val error: 21.840\n",
      "    9000 batch loss: 0.722 train error: 21.434 val error: 21.540\n",
      "train set model [ h = 3000  ], lambda= 0.0000 ] --> train error: 21.43, val error: 21.54\n",
      "best Loss is0.7035414215470124, best lambda is 0, best dropout rate is 0.1, best hidden size is3000\n"
     ]
    }
   ],
   "source": [
    "h_lst = [20, 200, 1000, 2000, 3000]\n",
    "l = 0.03\n",
    "d = 0.1 \n",
    "loss_lst = []\n",
    "val_error_lst = []\n",
    "train_error_lst = []\n",
    "\n",
    "for h in h_lst:\n",
    "    trainopt = {\n",
    "        'eta': 1e-3,   # initial learning rate\n",
    "        'maxiter': 10000,   # max number of iterations (updates) of SGD\n",
    "        'display_iter': 1000,  # display batch loss every display_iter updates, originally 5000\n",
    "        'batch_size': 128,  \n",
    "        'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "        'eta_frac': 0.5,  # drop eta after every eta_frac*maxiter\n",
    "        'update': 'sgd'\n",
    "    }\n",
    "    NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "    # we will maintain a record of models trained for different values of lambda\n",
    "    # these will be indexed directly by lambda value itself\n",
    "    trained_models = dict()\n",
    "\n",
    "    # choose the set of hyperparameters to explore\n",
    "\n",
    "    lambda_= l\n",
    "    hidden_size_=[h] # ADD CODE to specify hidden dim for each layer; \n",
    "    trainopt['lambda'] = lambda_\n",
    "    model = build_model(NFEATURES, hidden_size_, 10, dropout = d) \n",
    "    crit = SoftMaxLoss()\n",
    "    # -- model trained on large train set\n",
    "    trained_model,valErr,trainErr, loss = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "    trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr, 'loss': loss }\n",
    "    print('train set model [ h = ',end='')\n",
    "    for l in range(len(hidden_size_)):\n",
    "        print('%d '%hidden_size_[l],end='')\n",
    "    print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "    if best_loss == None:\n",
    "        best_loss = loss\n",
    "        best_lambda = l\n",
    "        best_h = h\n",
    "        best_dropout_rate = d\n",
    "    else:\n",
    "        if (best_loss > loss):\n",
    "            best_loss = loss\n",
    "            best_lambda = l\n",
    "            best_h = h\n",
    "            best_dropout_rate = d\n",
    "    loss_lst.append(loss)\n",
    "    train_error_lst.append(trainErr)\n",
    "    val_error_lst.append(valErr)\n",
    "print(\"best Loss is{}, best lambda is {}, best dropout rate is {}, best hidden size is{}\".format(best_loss, best_lambda, best_dropout_rate, best_h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = [20, 200, 1000, 2000, 3000]\n",
    "fig, axs = plt.subplots(1, 1, figsize=(12,10))\n",
    "\n",
    "loss_lst = [1.252, 0.995, 0.810, 0.772, 0.722]\n",
    "val_error_lst = [29.994, 27.932, 25.154, 23.142, 21.434]\n",
    "train_error_lst = [29.900, 27.760, 24.680, 23.020, 21.540]\n",
    "\n",
    "\n",
    "\n",
    "axs.plot(h, loss_lst, color = 'red', label = 'Loss')\n",
    "axs.plot(h, val_error_lst, color = 'blue', label = 'val_error')\n",
    "axs.plot(h, train_error_lst, color = 'green', label = 'train_error')\n",
    "\n",
    "plt.xlabel(\"Value of h\")\n",
    "plt.ylabel(\"Loss/Error\")\n",
    "\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameter for a two-layer network is  best lambda is 0.03, best dropout rate is 0.1, \n",
    "best hidden size is 3000. The lowest loss is 0.722, the best train error is 21.434 \n",
    "the best val error is 21.540."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.298 train error: 86.430 val error: 86.440\n",
      "    1000 batch loss: 1.988 train error: 57.260 val error: 57.360\n",
      "    2000 batch loss: 1.799 train error: 46.136 val error: 46.440\n",
      "    3000 batch loss: 1.567 train error: 40.000 val error: 40.520\n",
      "    4000 batch loss: 1.452 train error: 37.308 val error: 37.480\n",
      "    5000 batch loss: 1.364 train error: 35.640 val error: 35.980\n",
      "    6000 batch loss: 1.268 train error: 35.154 val error: 35.460\n",
      "    7000 batch loss: 1.291 train error: 34.704 val error: 35.020\n",
      "    8000 batch loss: 1.303 train error: 34.496 val error: 34.700\n",
      "    9000 batch loss: 1.221 train error: 34.346 val error: 34.520\n",
      "train set model [ h = 200 80  ], lambda= 0.0100 ] --> train error: 34.35, val error: 34.52\n",
      "       0 batch loss: 2.459 train error: 92.504 val error: 91.920\n",
      "    1000 batch loss: 1.991 train error: 53.796 val error: 54.120\n",
      "    2000 batch loss: 1.677 train error: 43.688 val error: 43.900\n",
      "    3000 batch loss: 1.564 train error: 38.408 val error: 38.380\n",
      "    4000 batch loss: 1.536 train error: 36.714 val error: 36.560\n",
      "    5000 batch loss: 1.436 train error: 35.530 val error: 35.680\n",
      "    6000 batch loss: 1.364 train error: 35.166 val error: 35.180\n",
      "    7000 batch loss: 1.412 train error: 34.848 val error: 34.920\n",
      "    8000 batch loss: 1.339 train error: 34.640 val error: 34.780\n",
      "    9000 batch loss: 1.249 train error: 34.530 val error: 34.580\n",
      "train set model [ h = 200 80  ], lambda= 0.0300 ] --> train error: 34.53, val error: 34.58\n",
      "       0 batch loss: 2.459 train error: 91.982 val error: 92.160\n",
      "    1000 batch loss: 2.238 train error: 51.240 val error: 50.160\n",
      "    2000 batch loss: 2.010 train error: 43.236 val error: 42.520\n",
      "    3000 batch loss: 1.901 train error: 41.004 val error: 40.260\n",
      "    4000 batch loss: 1.758 train error: 39.756 val error: 39.000\n",
      "    5000 batch loss: 1.693 train error: 38.334 val error: 37.920\n",
      "    6000 batch loss: 1.610 train error: 37.884 val error: 37.520\n",
      "    7000 batch loss: 1.653 train error: 37.504 val error: 37.220\n",
      "    8000 batch loss: 1.723 train error: 37.170 val error: 37.020\n",
      "    9000 batch loss: 1.638 train error: 37.004 val error: 36.700\n",
      "train set model [ h = 200 80  ], lambda= 0.0100 ] --> train error: 37.00, val error: 36.70\n",
      "       0 batch loss: 2.385 train error: 93.454 val error: 93.440\n",
      "    1000 batch loss: 2.124 train error: 61.928 val error: 62.840\n",
      "    2000 batch loss: 2.083 train error: 49.948 val error: 50.420\n",
      "    3000 batch loss: 1.984 train error: 44.180 val error: 43.860\n",
      "    4000 batch loss: 1.897 train error: 41.788 val error: 41.640\n",
      "    5000 batch loss: 1.839 train error: 40.060 val error: 39.880\n",
      "    6000 batch loss: 1.886 train error: 39.274 val error: 39.160\n",
      "    7000 batch loss: 1.746 train error: 38.728 val error: 38.540\n",
      "    8000 batch loss: 1.749 train error: 38.268 val error: 38.140\n",
      "    9000 batch loss: 1.779 train error: 38.018 val error: 38.000\n",
      "train set model [ h = 200 80  ], lambda= 0.0300 ] --> train error: 38.02, val error: 38.00\n",
      "       0 batch loss: 2.362 train error: 93.202 val error: 92.640\n",
      "    1000 batch loss: 2.015 train error: 54.716 val error: 55.280\n",
      "    2000 batch loss: 1.812 train error: 44.380 val error: 44.960\n",
      "    3000 batch loss: 1.591 train error: 39.876 val error: 40.780\n",
      "    4000 batch loss: 1.496 train error: 38.032 val error: 38.880\n",
      "    5000 batch loss: 1.410 train error: 36.404 val error: 36.240\n",
      "    6000 batch loss: 1.351 train error: 35.904 val error: 35.820\n",
      "    7000 batch loss: 1.358 train error: 35.496 val error: 35.280\n",
      "    8000 batch loss: 1.225 train error: 35.256 val error: 35.100\n",
      "    9000 batch loss: 1.284 train error: 35.090 val error: 35.100\n",
      "train set model [ h = 200 50  ], lambda= 0.0100 ] --> train error: 35.09, val error: 35.10\n",
      "       0 batch loss: 2.320 train error: 88.318 val error: 88.100\n",
      "    1000 batch loss: 2.067 train error: 63.724 val error: 64.160\n",
      "    2000 batch loss: 1.803 train error: 50.380 val error: 50.940\n",
      "    3000 batch loss: 1.690 train error: 43.670 val error: 44.300\n",
      "    4000 batch loss: 1.528 train error: 38.776 val error: 39.300\n",
      "    5000 batch loss: 1.475 train error: 36.364 val error: 36.740\n",
      "    6000 batch loss: 1.388 train error: 35.468 val error: 36.020\n",
      "    7000 batch loss: 1.445 train error: 34.922 val error: 35.500\n",
      "    8000 batch loss: 1.443 train error: 34.580 val error: 34.960\n",
      "    9000 batch loss: 1.435 train error: 34.424 val error: 34.800\n",
      "train set model [ h = 200 50  ], lambda= 0.0300 ] --> train error: 34.42, val error: 34.80\n",
      "       0 batch loss: 2.473 train error: 92.608 val error: 92.720\n",
      "    1000 batch loss: 2.199 train error: 74.614 val error: 73.840\n",
      "    2000 batch loss: 2.047 train error: 51.722 val error: 52.540\n",
      "    3000 batch loss: 2.005 train error: 41.916 val error: 42.380\n",
      "    4000 batch loss: 1.846 train error: 39.010 val error: 39.560\n",
      "    5000 batch loss: 1.844 train error: 37.662 val error: 38.220\n",
      "    6000 batch loss: 1.768 train error: 37.316 val error: 37.840\n",
      "    7000 batch loss: 1.774 train error: 36.948 val error: 37.220\n",
      "    8000 batch loss: 1.776 train error: 36.786 val error: 37.200\n",
      "    9000 batch loss: 1.728 train error: 36.594 val error: 37.160\n",
      "train set model [ h = 200 50  ], lambda= 0.0100 ] --> train error: 36.59, val error: 37.16\n",
      "       0 batch loss: 2.410 train error: 90.968 val error: 91.420\n",
      "    1000 batch loss: 2.180 train error: 74.332 val error: 75.940\n",
      "    2000 batch loss: 2.005 train error: 55.114 val error: 56.240\n",
      "    3000 batch loss: 1.957 train error: 46.252 val error: 46.840\n",
      "    4000 batch loss: 1.942 train error: 43.806 val error: 44.720\n",
      "    5000 batch loss: 1.797 train error: 42.148 val error: 42.860\n",
      "    6000 batch loss: 1.840 train error: 41.502 val error: 42.460\n",
      "    7000 batch loss: 1.828 train error: 41.058 val error: 41.660\n",
      "    8000 batch loss: 1.794 train error: 40.508 val error: 41.040\n",
      "    9000 batch loss: 1.761 train error: 40.226 val error: 40.660\n",
      "train set model [ h = 200 50  ], lambda= 0.0300 ] --> train error: 40.23, val error: 40.66\n",
      "       0 batch loss: 2.338 train error: 89.968 val error: 90.080\n",
      "    1000 batch loss: 2.040 train error: 59.014 val error: 60.480\n",
      "    2000 batch loss: 1.723 train error: 43.126 val error: 44.360\n",
      "    3000 batch loss: 1.517 train error: 37.742 val error: 38.980\n",
      "    4000 batch loss: 1.381 train error: 36.024 val error: 37.080\n",
      "    5000 batch loss: 1.382 train error: 34.898 val error: 35.860\n",
      "    6000 batch loss: 1.307 train error: 34.586 val error: 35.400\n",
      "    7000 batch loss: 1.254 train error: 34.232 val error: 34.960\n",
      "    8000 batch loss: 1.212 train error: 33.970 val error: 34.780\n",
      "    9000 batch loss: 1.202 train error: 33.840 val error: 34.460\n",
      "train set model [ h = 500 80  ], lambda= 0.0100 ] --> train error: 33.84, val error: 34.46\n",
      "       0 batch loss: 2.321 train error: 88.024 val error: 87.640\n",
      "    1000 batch loss: 1.918 train error: 44.302 val error: 44.660\n",
      "    2000 batch loss: 1.633 train error: 37.610 val error: 37.980\n",
      "    3000 batch loss: 1.494 train error: 36.190 val error: 36.120\n",
      "    4000 batch loss: 1.381 train error: 35.550 val error: 35.460\n",
      "    5000 batch loss: 1.359 train error: 34.834 val error: 34.780\n",
      "    6000 batch loss: 1.259 train error: 34.558 val error: 34.540\n",
      "    7000 batch loss: 1.279 train error: 34.332 val error: 34.380\n",
      "    8000 batch loss: 1.322 train error: 34.084 val error: 34.280\n",
      "    9000 batch loss: 1.330 train error: 33.940 val error: 34.260\n",
      "train set model [ h = 500 80  ], lambda= 0.0300 ] --> train error: 33.94, val error: 34.26\n",
      "       0 batch loss: 2.396 train error: 89.986 val error: 90.040\n",
      "    1000 batch loss: 2.100 train error: 55.926 val error: 54.820\n",
      "    2000 batch loss: 2.024 train error: 43.130 val error: 43.200\n",
      "    3000 batch loss: 1.874 train error: 38.104 val error: 38.180\n",
      "    4000 batch loss: 1.787 train error: 36.880 val error: 36.840\n",
      "    5000 batch loss: 1.641 train error: 36.130 val error: 36.160\n",
      "    6000 batch loss: 1.728 train error: 35.984 val error: 35.940\n",
      "    7000 batch loss: 1.709 train error: 35.582 val error: 35.640\n",
      "    8000 batch loss: 1.653 train error: 35.474 val error: 35.660\n",
      "    9000 batch loss: 1.522 train error: 35.422 val error: 35.480\n",
      "train set model [ h = 500 80  ], lambda= 0.0100 ] --> train error: 35.42, val error: 35.48\n",
      "       0 batch loss: 2.328 train error: 90.010 val error: 89.400\n",
      "    1000 batch loss: 2.011 train error: 63.770 val error: 63.940\n",
      "    2000 batch loss: 1.907 train error: 49.776 val error: 50.080\n",
      "    3000 batch loss: 1.799 train error: 42.682 val error: 43.120\n",
      "    4000 batch loss: 1.714 train error: 40.276 val error: 41.080\n",
      "    5000 batch loss: 1.652 train error: 38.512 val error: 39.100\n",
      "    6000 batch loss: 1.557 train error: 37.940 val error: 38.520\n",
      "    7000 batch loss: 1.565 train error: 37.436 val error: 38.000\n",
      "    8000 batch loss: 1.573 train error: 37.112 val error: 37.540\n",
      "    9000 batch loss: 1.569 train error: 36.834 val error: 37.120\n",
      "train set model [ h = 500 80  ], lambda= 0.0300 ] --> train error: 36.83, val error: 37.12\n",
      "       0 batch loss: 2.344 train error: 89.198 val error: 88.560\n",
      "    1000 batch loss: 2.024 train error: 46.488 val error: 47.500\n",
      "    2000 batch loss: 1.783 train error: 41.074 val error: 41.840\n",
      "    3000 batch loss: 1.507 train error: 37.492 val error: 37.740\n",
      "    4000 batch loss: 1.467 train error: 35.758 val error: 35.520\n",
      "    5000 batch loss: 1.311 train error: 34.574 val error: 34.480\n",
      "    6000 batch loss: 1.362 train error: 34.052 val error: 34.140\n",
      "    7000 batch loss: 1.347 train error: 33.684 val error: 33.840\n",
      "    8000 batch loss: 1.245 train error: 33.464 val error: 33.600\n",
      "    9000 batch loss: 1.219 train error: 33.232 val error: 33.340\n",
      "train set model [ h = 500 50  ], lambda= 0.0100 ] --> train error: 33.23, val error: 33.34\n",
      "       0 batch loss: 2.341 train error: 87.200 val error: 88.200\n",
      "    1000 batch loss: 1.936 train error: 51.880 val error: 51.820\n",
      "    2000 batch loss: 1.755 train error: 50.252 val error: 50.660\n",
      "    3000 batch loss: 1.592 train error: 45.038 val error: 44.400\n",
      "    4000 batch loss: 1.563 train error: 38.154 val error: 37.920\n",
      "    5000 batch loss: 1.419 train error: 35.168 val error: 35.040\n",
      "    6000 batch loss: 1.461 train error: 34.610 val error: 34.480\n",
      "    7000 batch loss: 1.454 train error: 34.192 val error: 34.120\n",
      "    8000 batch loss: 1.338 train error: 34.028 val error: 33.800\n",
      "    9000 batch loss: 1.335 train error: 33.896 val error: 33.680\n",
      "train set model [ h = 500 50  ], lambda= 0.0300 ] --> train error: 33.90, val error: 33.68\n",
      "       0 batch loss: 2.406 train error: 90.788 val error: 90.580\n",
      "    1000 batch loss: 2.123 train error: 54.046 val error: 54.340\n",
      "    2000 batch loss: 1.891 train error: 40.830 val error: 40.580\n",
      "    3000 batch loss: 1.844 train error: 38.636 val error: 38.260\n",
      "    4000 batch loss: 1.725 train error: 37.828 val error: 37.460\n",
      "    5000 batch loss: 1.556 train error: 37.098 val error: 36.620\n",
      "    6000 batch loss: 1.612 train error: 36.774 val error: 36.540\n",
      "    7000 batch loss: 1.760 train error: 36.552 val error: 36.240\n",
      "    8000 batch loss: 1.524 train error: 36.386 val error: 36.040\n",
      "    9000 batch loss: 1.601 train error: 36.282 val error: 36.000\n",
      "train set model [ h = 500 50  ], lambda= 0.0100 ] --> train error: 36.28, val error: 36.00\n",
      "       0 batch loss: 2.400 train error: 86.962 val error: 86.960\n",
      "    1000 batch loss: 2.080 train error: 51.200 val error: 51.620\n",
      "    2000 batch loss: 1.934 train error: 42.420 val error: 42.460\n",
      "    3000 batch loss: 1.750 train error: 40.010 val error: 39.800\n",
      "    4000 batch loss: 1.610 train error: 38.222 val error: 37.800\n",
      "    5000 batch loss: 1.676 train error: 36.762 val error: 36.300\n",
      "    6000 batch loss: 1.625 train error: 36.172 val error: 35.940\n",
      "    7000 batch loss: 1.675 train error: 35.754 val error: 35.440\n",
      "    8000 batch loss: 1.564 train error: 35.406 val error: 35.240\n",
      "    9000 batch loss: 1.537 train error: 35.288 val error: 34.980\n",
      "train set model [ h = 500 50  ], lambda= 0.0300 ] --> train error: 35.29, val error: 34.98\n",
      "best Loss is1.209570016903422, best lambda is 1, best dropout rate is [500, 80], best hidden size is0.1\n"
     ]
    }
   ],
   "source": [
    "dropout_lst = [0.1, 0.5]\n",
    "lambda_lst = [0.01, 0.03]\n",
    "hidden_one = [200, 80]\n",
    "hidden_two = [200, 50]\n",
    "hidden_three = [500, 80]\n",
    "hidden_four = [500, 50]\n",
    "h_lst = [hidden_one, hidden_two, hidden_three, hidden_four]\n",
    "\n",
    "best_loss = None\n",
    "best_h = 0\n",
    "best_dropout_rate = 0\n",
    "best_lambda = 0\n",
    "\n",
    "\n",
    "for h in h_lst:\n",
    "    for d in dropout_lst:\n",
    "        for l in lambda_lst:\n",
    "            trainopt = {\n",
    "                'eta': 1e-3,   # initial learning rate\n",
    "                'maxiter': 10000,   # max number of iterations (updates) of SGD\n",
    "                'display_iter': 1000,  # display batch loss every display_iter updates, originally 5000\n",
    "                'batch_size': 128,  \n",
    "                'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "                'eta_frac': .25,  # drop eta after every eta_frac*maxiter\n",
    "                'update': 'sgd'\n",
    "            }\n",
    "            NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "            # we will maintain a record of models trained for different values of lambda\n",
    "            # these will be indexed directly by lambda value itself\n",
    "            trained_models = dict()\n",
    "\n",
    "            # choose the set of hyperparameters to explore\n",
    "            lambda_= l\n",
    "            hidden_size_=h # ADD CODE to specify hidden dim for each layer; \n",
    "            trainopt['lambda'] = lambda_\n",
    "            model = build_model(NFEATURES, hidden_size_, 10, dropout = d) \n",
    "            crit = SoftMaxLoss()\n",
    "            # -- model trained on large train set\n",
    "            trained_model,valErr,trainErr, loss = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "            trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr, 'loss': loss }\n",
    "            print('train set model [ h = ',end='')\n",
    "            for l in range(len(hidden_size_)):\n",
    "                print('%d '%hidden_size_[l],end='')\n",
    "            print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "            if best_loss == None:\n",
    "                best_loss = loss\n",
    "                best_lambda = l\n",
    "                best_h = h\n",
    "                best_dropout_rate = d\n",
    "            else:\n",
    "                if (best_loss > loss):\n",
    "                    best_loss = loss\n",
    "                    best_lambda = l\n",
    "                    best_h = h\n",
    "                    best_dropout_rate = d\n",
    "print(\"best Loss is{}, best lambda is {}, best dropout rate is {}, best hidden size is{}\".format(best_loss, best_lambda, best_h, best_dropout_rate))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.373 train error: 86.986 val error: 86.980\n",
      "    1000 batch loss: 1.496 train error: 34.278 val error: 34.720\n",
      "    2000 batch loss: 1.179 train error: 32.820 val error: 32.360\n",
      "    3000 batch loss: 1.053 train error: 31.194 val error: 30.480\n",
      "    4000 batch loss: 0.973 train error: 30.092 val error: 29.200\n",
      "    5000 batch loss: 0.955 train error: 29.296 val error: 28.480\n",
      "    6000 batch loss: 0.985 train error: 28.728 val error: 27.980\n",
      "    7000 batch loss: 0.969 train error: 28.426 val error: 27.880\n",
      "    8000 batch loss: 0.921 train error: 28.036 val error: 27.560\n",
      "    9000 batch loss: 0.972 train error: 27.912 val error: 27.340\n",
      "train set model [ h = 3000 800  ], lambda= 0.0300 ] --> train error: 27.91, val error: 27.34\n",
      "best Loss is0.9532290671308266, best lambda is 1, best dropout rate is [3000, 800], best hidden size is0.1\n"
     ]
    }
   ],
   "source": [
    "h = [3000, 800]\n",
    "l = 0.03\n",
    "d = 0.1 \n",
    "loss_lst = []\n",
    "val_error_lst = []\n",
    "train_error_lst = []\n",
    "\n",
    "best_loss = None\n",
    "best_h = 0\n",
    "best_dropout_rate = 0\n",
    "best_lambda = 0\n",
    "\n",
    "trainopt = {\n",
    "    'eta': 1e-3,   # initial learning rate\n",
    "    'maxiter': 10000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 1000,  # display batch loss every display_iter updates, originally 5000\n",
    "    'batch_size': 128,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25,  # drop eta after every eta_frac*maxiter\n",
    "    'update': 'sgd'\n",
    "}\n",
    "NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "# we will maintain a record of models trained for different values of lambda\n",
    "# these will be indexed directly by lambda value itself\n",
    "trained_models = dict()\n",
    "\n",
    "\n",
    "# choose the set of hyperparameters to explore\n",
    "lambda_= l\n",
    "hidden_size_=h # ADD CODE to specify hidden dim for each layer; \n",
    "trainopt['lambda'] = lambda_\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = d) \n",
    "crit = SoftMaxLoss()\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr, loss = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr, 'loss': loss }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "if best_loss == None:\n",
    "    best_loss = loss\n",
    "    best_lambda = l\n",
    "    best_h = h\n",
    "    best_dropout_rate = d\n",
    "else:\n",
    "    if (best_loss > loss):\n",
    "        best_loss = loss\n",
    "        best_lambda = l\n",
    "        best_h = h\n",
    "        best_dropout_rate = d\n",
    "print(\"best Loss is{}, best lambda is {}, best dropout rate is {}, best hidden size is{}\".format(best_loss, best_lambda, best_h, best_dropout_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best lambda is 0.03. The best droput rate is 0.1. The best hidden size is \n",
    "[3000, 800]. The best loss is 0.953. The best train error is 27.91,\n",
    "the best val error: 27.34."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a Kaggle submission file using best_trained_model which you should set based on your experiments\n",
    "kaggleX = FMNIST_utils.load_data('kaggle')\n",
    "if kaggleX.max() > 1: kaggleX = kaggleX/255\n",
    "kaggleYhat = predict(kaggleX, trained_model).argmax(-1)\n",
    "save_submission('submission-fmnist.csv', kaggleYhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f594f625523eedf37de17555ec746ece4289f4fa39150ea03385bfab3da2cc11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
