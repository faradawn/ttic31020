{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import struct\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from typing import Union, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = str(pathlib.Path().absolute())\n",
    "## images path\n",
    "mnist_data = base_path+\"/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MNIST(dataset: str=\"training\", path: str=\"./\") -> Union[Tuple[np.ndarray,np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reads in the MNIST data\n",
    "    This function allows specification of the part to be read (training/testing/validation)\n",
    "    Note that if dataset='testing', no labels will be returned\n",
    "    \"\"\"\n",
    "    \n",
    "    #Figure out the name of the file to load    \n",
    "    if dataset.lower() == \"training\":\n",
    "        file_name_suffix = 'train'\n",
    "        has_labels = True\n",
    "    \n",
    "    elif dataset.lower() == \"validation\":\n",
    "        file_name_suffix = 'val'\n",
    "        has_labels = True\n",
    "\n",
    "    elif dataset.lower() == \"testing\":\n",
    "        file_name_suffix = 'test'\n",
    "        has_labels = True\n",
    "\n",
    "    else:\n",
    "        print(\"dataset must be 'testing','validation', or 'training'\")\n",
    "        raise ValueError\n",
    "    \n",
    "    #Load the appropriate files\n",
    "    X = np.load(mnist_data+'X'+file_name_suffix+'.npy')\n",
    "    if has_labels:\n",
    "        y = np.load(mnist_data+'y'+file_name_suffix+'.npy')\n",
    "    \n",
    "    X = np.float64(X).reshape(X.shape[0],-1)\n",
    "    y = y.flatten().astype('int')\n",
    "    \n",
    "    #Return the appropriate data\n",
    "    if has_labels:\n",
    "        return X,y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "\n",
    "def show_MNIST_example(image) -> None:\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 array of pixel data as an image.\n",
    "    \"\"\"\n",
    "    image = image[-28**2:]\n",
    "    image = image.reshape(28,28)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training image\n",
      "Class: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# load the data and visualize a training example\n",
    "X,y = read_MNIST(\"training\")\n",
    "print(\"Training image\")\n",
    "print(f\"Class: {y[100]}\")\n",
    "show_MNIST_example(X[100])\n",
    "\n",
    "X_val,y_val = read_MNIST(\"validation\")\n",
    "X_test, y_test = read_MNIST(\"testing\")\n",
    "\n",
    "# let's shuffle the data\n",
    "np.random.seed(789123)\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "print(type(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees (ToDo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Building block of the Decision Tree.\n",
    "    \"\"\"\n",
    "    left = None  # left child\n",
    "    right = None  # right child\n",
    "    index = None  # feature index (what is the feature by which we split)\n",
    "    val = None  # what's the value at which we split\n",
    "    # Once you arrive at a leaf Node then we use labels\n",
    "    is_leaf = True\n",
    "    label = None\n",
    "    \n",
    "    def __init__(self, left=None, right=None, index=None, val=None, is_leaf=True, label=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.val = val\n",
    "        self.is_leaf = is_leaf\n",
    "        self.label = label\n",
    "        \n",
    "    def classify(self, x):\n",
    "        # this is a recursive function, following the path to the correct leaf\n",
    "        if self.is_leaf:\n",
    "            return self.label\n",
    "        elif x[self.index] <= self.val:\n",
    "            return self.left.classify(x)\n",
    "        elif x[self.index] > self.val:\n",
    "            return self.right.classify(x)\n",
    "        \n",
    "            \n",
    "    def classify_set(self, X):\n",
    "        # work on examples in a loop \n",
    "        res = []\n",
    "        for sample in X:\n",
    "            res.append(self.classify(sample))\n",
    "        return np.asarray(res)\n",
    "            \n",
    "    #useful for debugging (on small trees)\n",
    "    def print_tree(self) -> None:\n",
    "        print(f\"index = {self.index}, val = {self.val}, leaf = {self.is_leaf}, label = {self.label}, left = {self.left}, right = {self.right}\")\n",
    "        if self.left is not None:\n",
    "            print(\"left = \", end=\"\")\n",
    "            self.left.print_tree()\n",
    "        if self.right is not None:\n",
    "            print(\"right = \", end=\"\")\n",
    "            self.right.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to classify a leaf node take the mode of the labels that arrive at the leaf\n",
    "def label_leaf(labels):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    return classes[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split_gini(features, labels, min_split_size=5):\n",
    "    \"\"\"\n",
    "    Given a set of feature values (for a single feature) and the corresponding labels,\n",
    "    find the split that minimizes the (weighted by size) sum of the Gini indices of the two parts\n",
    "    Note: the combined Gini index is given by len(L)*G[0]+len(U)*G[1]\n",
    "\n",
    "    Returns: (tau, G, L, U)\n",
    "        tau : the threshold to split\n",
    "        G : 2-tuple with the Gini indices of the two parts\n",
    "        L, U : indices of the points that go into L and U\n",
    "    \"\"\"\n",
    "    \n",
    "    # sort the feature values (and permute the labels to match that order)\n",
    "    order = np.argsort(features)\n",
    "    x = features[order]\n",
    "    y = labels[order]\n",
    "    N = len(x)\n",
    "    classes = np.unique(labels)\n",
    "    \n",
    "    # check if there is nothing to split\n",
    "    if (x[0]==x[-1]) or (len(classes)==1):\n",
    "        return None, [np.inf,np.inf], range(N), None    \n",
    "    \n",
    "    # freq_u maintains frequencies of labels in the (potential) upper portion of the split,\n",
    "    # and freq_l in the lower\n",
    "    \n",
    "    freq_u = np.zeros(np.max(classes) + 1)\n",
    "    for c in range(len(freq_u)):\n",
    "        freq_u[c] = np.mean(labels==c)\n",
    "    freq_l = np.zeros(np.max(classes) + 1)\n",
    "    \n",
    "    # now go over the examples in order of increasing feature value;\n",
    "    # consider splitting before each index where the value changes\n",
    "    # rather that computing the Gini index from scratch for every split, we will keep track\n",
    "    # of the running frequencies (as we traverse the list of examples ordered by feature value)\n",
    "    # and simply recompute Gini from the frequencies each step (much faster)\n",
    "    \n",
    "    gini=dict()\n",
    "    splits=dict()\n",
    "    for i in range(N-1):\n",
    "        # update frequencies\n",
    "        freq_l[y[i]] += 1/N     # This data is removed from the lower range\n",
    "                                # into the upper range\n",
    "        freq_u[y[i]] -= 1/N\n",
    "        \n",
    "        if x[i+1] > x[i]: # split here, between i and i+1?\n",
    "            tau = (x[i+1] + x[i]) / 2.0  # a threshold that separates \n",
    "                                         # x[i+1] and x[i]\n",
    "            \n",
    "            gini_l = np.dot(freq_l, 1-freq_l)\n",
    "            gini_u = np.dot(freq_u, 1-freq_u)\n",
    "            \n",
    "            gini[tau] = (i * gini_l + (N-i) * gini_u, gini_l, gini_u) \n",
    "            splits[tau] = i+1 # index of the first element that goes to U\n",
    "\n",
    "    # now find the best split\n",
    "    best_tau = -1\n",
    "    bestval = np.inf\n",
    "    for t in gini:\n",
    "        if gini[t][0] < bestval:\n",
    "            best_tau = t\n",
    "            bestval = gini[t][0]\n",
    "    \n",
    "    L = order[:splits[best_tau]]\n",
    "    U = order[splits[best_tau]:]\n",
    "    \n",
    "    return best_tau, gini[best_tau][1:], L, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, min_split_size=5, M=None):\n",
    "    # Find the best way to split the data in X (with labels y) by a single feature\n",
    "    # Don't split if not enough data given min_split_size (return None), otherwise\n",
    "    # return a dictionary with the following key/value pairs:\n",
    "    # 'd' -> the index of the feature in X by which we are splitting;\n",
    "    # 'tau' -> the threshold at which we are splitting that feature;\n",
    "    # 'Gini' -> the (weighted) Gini index of the two new leaves;\n",
    "    # 'Gini-parts' -> a 2-tuple (list) of Gini indices of the L and U leaves after the split;\n",
    "    # 'L' -> indices of the examples in X that go into the lower (left) leaf;\n",
    "    # 'U' -> same for the upper (right) leaf\n",
    "    #\n",
    "    # M specifies the number of features to consider in looking for the best split. If M=None, this means using\n",
    "    # all the features of X. Otherwise, if say M=50, this means pick 50 features at random and only consider \n",
    "    # splitting by these features.\n",
    "    \n",
    "    if X.shape[0] < min_split_size:\n",
    "        return None\n",
    "    \n",
    "    D = X.shape[1] # number of features\n",
    "\n",
    "    if M == None:\n",
    "        f_lst = [i for i in range(D)]\n",
    "    else:\n",
    "        lst = [i for i in range(D)]\n",
    "        f_lst = [i for i in random.sample(lst, 50)] # randomly get 50 feature index\n",
    "\n",
    "    min_weighted_gini = None\n",
    "    best_feature_id = None\n",
    "    best_feature_tau = None\n",
    "    best_gini_parts = None\n",
    "    best_lower_index = None\n",
    "    best_upper_index = None\n",
    "\n",
    "    for id in f_lst:\n",
    "        # id here represents a single feature\n",
    "        features = X[: , id] #Set of feature values\n",
    "        tau, gini_parts, L, U = best_split_gini(features, y, min_split_size)\n",
    "\n",
    "        if (tau == None):\n",
    "            # when there is nothing to split \n",
    "            continue\n",
    "\n",
    "        num_lower = len(L)\n",
    "        num_upper = len(U)\n",
    "        weighted_gini = (gini_parts[0] * num_lower + gini_parts[1] * num_upper) / (num_lower + num_upper)\n",
    "\n",
    "        if (min_weighted_gini == None) or (weighted_gini < min_weighted_gini):\n",
    "            min_weighted_gini = weighted_gini\n",
    "            best_feature_id = id\n",
    "            best_feature_tau = tau\n",
    "            best_gini_parts = gini_parts\n",
    "            best_lower_index = L\n",
    "            best_upper_index = U\n",
    "    \n",
    "    if best_feature_id == None:\n",
    "        # For all selected features, there is nothing left to split\n",
    "        return None\n",
    "\n",
    "\n",
    "    res = dict()\n",
    "    res['d'] = best_feature_id\n",
    "    res['tau'] = best_feature_tau\n",
    "    res['Gini'] = min_weighted_gini\n",
    "    res['Gini-parts'] = best_gini_parts\n",
    "    res['L'] = best_lower_index\n",
    "    res['U'] = best_upper_index\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_tree(\n",
    "    root, \n",
    "    data, \n",
    "    labels, \n",
    "    depth, \n",
    "    max_depth=np.inf, \n",
    "    min_split_size=1, \n",
    "    M=None):\n",
    "    # this takes root (a node associated with data/labels, and existing at the given depth)\n",
    "    # and builds a tree rooted at that node, not to exceed the given max_depth\n",
    "   \n",
    "    # take a look at Node.classify() to get a better sense how the recursive nature of the tree\n",
    "    # lets you \"drop a point down the tree\" and assign a label once you are in the leaf\n",
    "\n",
    "    if (depth == max_depth):\n",
    "        no_split=True # we are done, no more splitting below this node\n",
    "    else:\n",
    "        best_split = split_data(data, labels, min_split_size,M)\n",
    "        if best_split==None: # for some reason the splitting code refused to split\n",
    "            no_split=True\n",
    "        else: # splitting!\n",
    "            no_split=False\n",
    "            \n",
    "            # COMPLETE CODE: assign values to .index, .val, .is_leaf\n",
    "            \n",
    "            root.left = grow_tree(Node(), data[best_split['L']], labels[best_split['L']], depth+1, max_depth,min_split_size,M)\n",
    "            root.right = grow_tree(Node(), data[best_split['U']], labels[best_split['U']], depth+1, max_depth,min_split_size,M)\n",
    "            root.index = best_split['d']\n",
    "            root.val = best_split['tau']\n",
    "            root.is_leaf = False\n",
    "            \n",
    "            \n",
    "    if no_split: # this is going to be a leaf, let's record this fact and assign it a label to predict\n",
    "        # COMPLETE CODE HERE  -- what do you need to assign in a leaf?\n",
    "        root.is_leaf = True\n",
    "        root.label = label_leaf(labels)\n",
    "               \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, labels, max_depth=np.inf, min_split_size=1, M=None):\n",
    "    root=Node()\n",
    "    tree=grow_tree(root,\n",
    "                   data,\n",
    "                   labels,\n",
    "                   depth=0, # this will \"signal\" to the tree growing function that this is the root\n",
    "                   max_depth=max_depth,\n",
    "                   min_split_size=min_split_size,\n",
    "                   M=M)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = build_tree(X, y, max_depth=10, min_split_size=3, M=None) # may take a few minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation accuracy for the tree (ToDo)\n",
    "\n",
    "- **Compute** training and validation accuracies\n",
    "\n",
    "- **Report** training and validation set accuracies of a single tree, \n",
    "    - with **hyperparameters** (max tree depth and minimum size of a leaf) tuned as you see fit (make sure to describe how you tuned them and include the code and output from the process in the notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train accuracy is 0.9096444444444445\n"
     ]
    }
   ],
   "source": [
    "predict = tree.classify_set(X)\n",
    "num_correct = 0\n",
    "num_total = len(predict)\n",
    "for i in range(num_total):\n",
    "    if (y[i] == predict[i]):\n",
    "        num_correct += 1\n",
    "train_accuracy = num_correct/num_total\n",
    "print(\"The train accuracy is {}\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 0.847, the best depth is 10, the best size is 3\n"
     ]
    }
   ],
   "source": [
    "max_depth = [5, 10, 15]\n",
    "split_size = [3, 5]\n",
    "best_accuracy = 0\n",
    "best_depth = 0\n",
    "best_size = 0\n",
    "for depth in max_depth:\n",
    "    for size in split_size:\n",
    "        tree = build_tree(X, y, max_depth=depth, min_split_size=size, M=None)\n",
    "        val_predict = tree.classify_set(X_val)\n",
    "        num_correct = 0\n",
    "        num_total = len(val_predict)\n",
    "        for i in range(num_total):\n",
    "            if (y_val[i] == val_predict[i]):\n",
    "                num_correct += 1\n",
    "        val_accuracy = num_correct/num_total\n",
    "\n",
    "        if(val_accuracy > best_accuracy):\n",
    "            best_accuracy = val_accuracy\n",
    "            best_depth = depth\n",
    "            best_size = size\n",
    "\n",
    "print(\"The best accuracy is {}, the best depth is {}, the best size is {}\".format(best_accuracy, best_depth, best_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predict = tree.classify_set(X_val)\n",
    "num_correct = 0\n",
    "num_total = len(val_predict)\n",
    "for i in range(num_total):\n",
    "    if (y_val[i] == val_predict[i]):\n",
    "        num_correct += 1\n",
    "val_accuracy = num_correct/num_total\n",
    "print(\"The accuracy on the val set is {}\".format(val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter we choose is max_depth = 10, min_split_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy (ToDo)\n",
    "\n",
    "- **Compute and report** the results on test set, for the model built with the chosen values – please don’t be tempted to evaluate the model on test more than once, so be sure you are done with the tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree(X, y, max_depth=10, min_split_size=3, M=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the val set is 0.8522\n"
     ]
    }
   ],
   "source": [
    "test_predict = tree.classify_set(X_test)\n",
    "num_correct = 0\n",
    "num_total = len(test_predict)\n",
    "for i in range(num_total):\n",
    "    if (y_test[i] == test_predict[i]):\n",
    "        num_correct += 1\n",
    "test_accuracy = num_correct/num_total\n",
    "print(\"The accuracy on the val set is {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the Tree (ToDo)\n",
    "\n",
    "- **Question 4 is a bonus question**. You are encouraged to do it\n",
    "\n",
    "- Add the code for pruning a tree\n",
    "- Report the accuracy of the CART tree (with pruning) using hyperparameters (including λ) tuned as you see fit, on the test set; \n",
    "- Compare it to the result with the unpruned tree, and comment on the difference, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse a tree according to the direction\n",
    "def traverse_tree(root, direction):\n",
    "    if direction == \"right\":\n",
    "        if (root.right == None):\n",
    "            print(\"go to a NULL node\")\n",
    "            exit(-1)\n",
    "        else:\n",
    "            root = root.right\n",
    "    else:\n",
    "        if (root.left == None):\n",
    "            print(\"go to a NULL node\")\n",
    "            exit(-1)\n",
    "        else:\n",
    "            root = root.left\n",
    "\n",
    "def check_internal_node(root):\n",
    "    # check whether the node is an internal node that can be collapsed\n",
    "    return root.left.is_leaf and root.right.is_leaf\n",
    "\n",
    "def calculate_leaf_error(root, data, labels):\n",
    "    error = 0\n",
    "    label = label_leaf(labels)\n",
    "    for k in range(len(data)):\n",
    "        error += (data[k] - label) ** 2\n",
    "\n",
    "def calculate_error_increase(root, data, labels):\n",
    "    if (root.is_leaf == True):\n",
    "        # if the node is alreay a leaf, nothing to collapse\n",
    "        return None\n",
    "    if check_internal_node(root):\n",
    "        # parent of two node, considering collapsing it\n",
    "        left_data = []\n",
    "        left_labels = []\n",
    "        right_data = []\n",
    "        right_labels = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            x = data[i]\n",
    "            y = labels[i]\n",
    "            if x[root.index] < root.val:\n",
    "                left_data.append(x)\n",
    "                left_labels.append(y)\n",
    "            else:\n",
    "                right_data.append(x)\n",
    "                right_labels.append(y)\n",
    "        left_label = label_leaf(left_labels)\n",
    "        right_label = label_leaf(right_labels)\n",
    "\n",
    "        original_error = 0\n",
    "        for i in range(len(left_data)):\n",
    "            original_error += ((left_data[i] - left_label) ** 2)\n",
    "        for j in range(len(right_data)):\n",
    "            original_error += ((right_data[j] - right_label) ** 2)\n",
    "        \n",
    "        new_error = 0\n",
    "        label = label_leaf(labels)\n",
    "        for k in range(len(data)):\n",
    "            new_error += (data[k] - label) ** 2\n",
    "\n",
    "        error_increase = new_error - original_error\n",
    "        res = (original_error, new_error, error_increase)\n",
    "        return res\n",
    "\n",
    "# collapse an internal node\n",
    "def prune_one_node(root, data, labels):\n",
    "    label = label_leaf(labels)\n",
    "\n",
    "    if ((root.left.is_leaf == True) and (root.right.is_leaf == True)):\n",
    "        root.left = None\n",
    "        root.right = None\n",
    "        root.index = None\n",
    "        root.val = None\n",
    "        root.is_leaf = True\n",
    "        root.label = label\n",
    "\n",
    "    return root\n",
    "\n",
    "def collapse_internal_node(root, data, labels):\n",
    "\n",
    "    # base cases\n",
    "    if root.is_leaf:\n",
    "        res = calculate_leaf_error(root)\n",
    "        return res\n",
    "\n",
    "    if check_internal_node(root.left) and root.right.is_leaf:\n",
    "        res_left = calculate_error_increase(root.left)\n",
    "        right_error = calculate_leaf_error(root.right)\n",
    "        res = res_left[1] + right_error\n",
    "        prune_one_node(root.left)\n",
    "        return res\n",
    "\n",
    "    if check_internal_node(root.right) and root.left.is_leaf:\n",
    "        res_right = calculate_error_increase(root.right)\n",
    "        left_error = calculate_leaf_error(root.left)\n",
    "        res = res_right[1] + left_error\n",
    "        prune_one_node(root.right)\n",
    "        return res\n",
    "    \n",
    "    if (check_internal_node(root.left) and check_internal_node(root.right)):\n",
    "        res_left = calculate_error_increase(root.left)\n",
    "        res_right = calculate_error_increase(root.right)\n",
    "\n",
    "        if (res_left[2] < res_right[2]):\n",
    "            # the increaes error is smaller if we prune the left tree\n",
    "            prune_one_node(root.left)\n",
    "            return (res_left[1] + res_right[0])\n",
    "        else:\n",
    "            prune_one_node(root.right)\n",
    "            return (res_right[1] + res_left[0])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if root.left.is_leaf and (root.right.is_leaf == False):\n",
    "        collapse_internal_node(root.right)\n",
    "\n",
    "    if root.right.is_leaf and (root.left.is_leaf == False):\n",
    "        collapse_internal_node(root.left)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for running bagging ensembles on data\n",
    "def apply_ensemble(trees, data, C=10):\n",
    "    \"\"\"Returns the labels after applying the ensemble\"\"\"\n",
    "\n",
    "    # apply every tree in the ensemble to classify the result and put it \n",
    "    # into a list, then each such list becomes an elemenent in the array\n",
    "    # res.shape[0] = number of trees\n",
    "    # res.shape[1] = number of samples \n",
    "    res = np.array([tree.classify_set(data) for tree in trees])\n",
    "    yhat = np.zeros(res.shape[1])\n",
    "    for i in range(res.shape[1]):\n",
    "        # for a sample, if it's most commonly been attributed to a certain class\n",
    "        # by the ensemble, count that data into that class\n",
    "        yhat[i]=np.argmax(np.bincount(res[:,i]))\n",
    "    return yhat.astype(int)\n",
    "\n",
    "def evaluate_ensemble(trees, data, labels):\n",
    "    \"\"\"Returns the accuracy of the ensemble\"\"\"\n",
    "    return np.mean(apply_ensemble(trees, data) == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ensemble (ToDo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, y):\n",
    "    N = X.shape[0]\n",
    "    lst = np.random.randint(N, size = N)\n",
    "    return X[lst], y[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "def create_ensemble(X, y, max_depth=10, min_split_size=1, num_trees=5, M_fraction=0.5):\n",
    "    \"\"\"\n",
    "    M_fraction gives M as a fraction of d (# of features in X)\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    assert 0 < M_fraction < 1.0\n",
    "    M = int(d*M_fraction)  # compute the actual M (number of features to consider in each node)\n",
    "    \n",
    "    trees = []\n",
    "    for _ in tqdm(range(num_trees)): # this will show progress bar as you build the forest\n",
    "        \n",
    "        # COMPLETE CODE HERE -- construct tree\n",
    "        X_sample, y_sample = bootstrap(X,y)\n",
    "        tree = build_tree(X_sample, y_sample, max_depth, min_split_size, M)\n",
    "        trees.append(tree)\n",
    "        \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB: This step takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [27:33<00:00, 55.12s/it]\n"
     ]
    }
   ],
   "source": [
    "ensemble = create_ensemble(X, y, max_depth=15, min_split_size=5, num_trees=30, M_fraction=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Accuracy (ToDo)\n",
    "\n",
    "- **Compute** the training and validation accuracy\n",
    "\n",
    "- **Report** the training and validation set accuracies of your ensemble. You should aim to get at least **90%** validation accuracy.\n",
    "- **Plot** the val accuracy as a function of ensemble size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.9973333333333333\n",
      "Val accuracy is 0.9518666666666666\n"
     ]
    }
   ],
   "source": [
    "train_acc = evaluate_ensemble(ensemble, X, y)\n",
    "val_acc = evaluate_ensemble(ensemble, X_val, y_val)\n",
    "\n",
    "print(\"Training accuracy is {}\".format(train_acc))\n",
    "print(\"Val accuracy is {}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning for a good value of M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [24:31<00:00, 49.07s/it]\n",
      "100%|██████████| 30/30 [25:57<00:00, 51.93s/it]\n",
      "100%|██████████| 30/30 [24:41<00:00, 49.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy rate on validation set is0.9536, the best M is0.1\n"
     ]
    }
   ],
   "source": [
    "M_lst = [0.01, 0.05, 0.1]\n",
    "\n",
    "best_M = 0\n",
    "best_acc = 0\n",
    "\n",
    "for m in M_lst:\n",
    "    ensemble = create_ensemble(X, y, max_depth=15, min_split_size=5, num_trees=30, M_fraction=m)\n",
    "    val_acc = evaluate_ensemble(ensemble, X_val, y_val)\n",
    "    if (val_acc > best_acc):\n",
    "        best_acc = val_acc\n",
    "        best_M = m\n",
    "print(\"The best accuracy rate on validation set is{}, the best M is{}\".format(best_acc, best_M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best accuracy achieved on the val set is 0.9536, and the best M is 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy (ToDo)\n",
    "- **Compute and report** the results of your ensemble on the test set. Again, don’t do it more than once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9564\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate_ensemble(ensemble, X_test, y_test)\n",
    "print(\"Test accuracy is {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test set is 0.9564."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of the size of your ensemble (ToDo)\n",
    "\n",
    "- **Show the effect of the size of your ensemble as follows**: \n",
    "    - Let T be the number of trees in your ensemble. \n",
    "    - For t = 1, . . . , T \n",
    "        - **Compute** the validation set accuracy of the partial ensemble consisting of trees 1, . . . , t\n",
    "        - **Plot** this as a function of t. \n",
    "- Discuss what this tells you and how it may inform tuning T.\n",
    "\n",
    "*Advice: Do not re-run the creation of the forest for T = 1, 2, . . .! rather, take your existing ensemble and compute the accuracy of the partial ensembles consisting of only the first tree; the first two trees; etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7934666666666667,\n",
       " 0.7848,\n",
       " 0.8517333333333333,\n",
       " 0.8826666666666667,\n",
       " 0.8978666666666667,\n",
       " 0.9094,\n",
       " 0.9170666666666667,\n",
       " 0.9222666666666667,\n",
       " 0.9252,\n",
       " 0.9281333333333334,\n",
       " 0.9328,\n",
       " 0.9364,\n",
       " 0.9382666666666667,\n",
       " 0.9404666666666667,\n",
       " 0.9416,\n",
       " 0.9432,\n",
       " 0.9448,\n",
       " 0.9456666666666667,\n",
       " 0.9466,\n",
       " 0.9482,\n",
       " 0.9493333333333334,\n",
       " 0.9494,\n",
       " 0.9509333333333333,\n",
       " 0.9508,\n",
       " 0.9510666666666666,\n",
       " 0.9517333333333333,\n",
       " 0.9526,\n",
       " 0.9536,\n",
       " 0.9534666666666667,\n",
       " 0.9536]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble\n",
    "\n",
    "sub_tree = []\n",
    "res_lst = []\n",
    "for i in range(len(ensemble)):\n",
    "    sub_tree.append(ensemble[i])\n",
    "    res = evaluate_ensemble(sub_tree, X_val, y_val)\n",
    "    res_lst.append(res)\n",
    "res_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Val Accuracy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(len(ensemble))]\n",
    "plt.scatter(x, res_lst)\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Val Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When T is small, the accuracy rate rises rapidly when T gets larger. This makes sense because each tree should have low bias but high variance. Hence, combining them will greatly reduce the variance and drive the validation accuracy higher.However, when T is sufficiently lagre (in our example, when T > 20), the accuracy rate starts to converge. Hence, when T gets too large, the algorithm wastes computation power instead of adding much to the accuracy rate. When tuning T, we need to start from a relatively small T, increase the number of T, and if the val accuracy rate converges, we should stop increasing T anymore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2a4012fca6975ccb4e2908886d96de946d5fe4261c9ff5ee375ddfd35668e36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
