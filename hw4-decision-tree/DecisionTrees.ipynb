{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import struct\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = str(pathlib.Path().absolute())\n",
    "## images path\n",
    "mnist_data = base_path+\"/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MNIST(dataset: str=\"training\", path: str=\"./\") -> Union[Tuple[np.ndarray,np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reads in the MNIST data\n",
    "    This function allows specification of the part to be read (training/testing/validation)\n",
    "    Note that if dataset='testing', no labels will be returned\n",
    "    \"\"\"\n",
    "    \n",
    "    #Figure out the name of the file to load    \n",
    "    if dataset.lower() == \"training\":\n",
    "        file_name_suffix = 'train'\n",
    "        has_labels = True\n",
    "    \n",
    "    elif dataset.lower() == \"validation\":\n",
    "        file_name_suffix = 'val'\n",
    "        has_labels = True\n",
    "\n",
    "    elif dataset.lower() == \"testing\":\n",
    "        file_name_suffix = 'test'\n",
    "        has_labels = True\n",
    "\n",
    "    else:\n",
    "        print(\"dataset must be 'testing','validation', or 'training'\")\n",
    "        raise ValueError\n",
    "    \n",
    "    #Load the appropriate files\n",
    "    X = np.load(mnist_data+'X'+file_name_suffix+'.npy')\n",
    "    if has_labels:\n",
    "        y = np.load(mnist_data+'y'+file_name_suffix+'.npy')\n",
    "    \n",
    "    X = np.float64(X).reshape(X.shape[0],-1)\n",
    "    y = y.flatten().astype('int')\n",
    "    \n",
    "    #Return the appropriate data\n",
    "    if has_labels:\n",
    "        return X,y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "\n",
    "def show_MNIST_example(image) -> None:\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 array of pixel data as an image.\n",
    "    \"\"\"\n",
    "    image = image[-28**2:]\n",
    "    image = image.reshape(28,28)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training image\n",
      "Class: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the data and visualize a training example\n",
    "X,y = read_MNIST(\"training\")\n",
    "print(\"Training image\")\n",
    "print(f\"Class: {y[10]}\")\n",
    "show_MNIST_example(X[10])\n",
    "\n",
    "X_val,y_val = read_MNIST(\"validation\")\n",
    "X_test, y_test = read_MNIST(\"testing\")\n",
    "\n",
    "# let's shuffle the data\n",
    "np.random.seed(789123)\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees (ToDo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Building block of the Decision Tree.\n",
    "    \"\"\"\n",
    "    left = None  # left child\n",
    "    right = None  # right child\n",
    "    index = None  # feature index (what is the feature by which we split)\n",
    "    val = None  # what's the value at which we split\n",
    "    # Once you arrive at a leaf Node then we use labels\n",
    "    is_leaf = True\n",
    "    label = None\n",
    "    \n",
    "    def __init__(self, left=None, right=None, index=None, val=None, is_leaf=True, label=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.val = val\n",
    "        self.is_leaf = is_leaf\n",
    "        self.label = label\n",
    "        \n",
    "    def classify(self, x):\n",
    "        # this is a recursive function, following the path to the correct leaf\n",
    "        if self.is_leaf:\n",
    "            return self.label\n",
    "        elif x[self.index] <= self.val:\n",
    "            return self.left.classify(x)\n",
    "        elif x[self.index] > self.val:\n",
    "            return self.right.classify(x)\n",
    "        \n",
    "            \n",
    "    def classify_set(self, X):\n",
    "        # work on examples in a loop \n",
    "        res = []\n",
    "        for sample in X:\n",
    "            res.append(self.classify(sample))\n",
    "        return np.asarray(res)\n",
    "            \n",
    "    #useful for debugging (on small trees)\n",
    "    def print_tree(self) -> None:\n",
    "        print(f\"index = {self.index}, val = {self.val}, leaf = {self.is_leaf}, label = {self.label}, left = {self.left}, right = {self.right}\")\n",
    "        if self.left is not None:\n",
    "            print(\"left = \", end=\"\")\n",
    "            self.left.print_tree()\n",
    "        if self.right is not None:\n",
    "            print(\"right = \", end=\"\")\n",
    "            self.right.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to classify a leaf node take the mode of the labels that arrive at the leaf\n",
    "def label_leaf(labels):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    return classes[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split_gini(features, labels, min_split_size=5):\n",
    "    \"\"\"\n",
    "    Given a set of feature values (for a single feature) and the corresponding labels,\n",
    "    find the split that minimizes the (weighted by size) sum of the Gini indices of the two parts\n",
    "    Note: the combined Gini index is given by len(L)*G[0]+len(U)*G[1]\n",
    "\n",
    "    Returns: (tau, G, L, U)\n",
    "        tau : the threshold to split\n",
    "        G : 2-tuple with the Gini indices of the two parts\n",
    "        L, U : indices of the points that go into L and U\n",
    "    \"\"\"\n",
    "    \n",
    "    # sort the feature values (and permute the labels to match that order)\n",
    "    order = np.argsort(features)\n",
    "    x = features[order]\n",
    "    y = labels[order]\n",
    "    N = len(x)\n",
    "    classes = np.unique(labels)\n",
    "    \n",
    "    # check if there is nothing to split\n",
    "    if (x[0]==x[-1]) or (len(classes)==1):\n",
    "        return None, [np.inf,np.inf], range(N), None    \n",
    "    \n",
    "    # freq_u maintains frequencies of labels in the (potential) upper portion of the split,\n",
    "    # and freq_l in the lower\n",
    "    \n",
    "    freq_u = np.zeros(np.max(classes) + 1)\n",
    "    for c in range(len(freq_u)):\n",
    "        freq_u[c] = np.mean(labels==c)\n",
    "    freq_l = np.zeros(np.max(classes) + 1)\n",
    "    \n",
    "    # now go over the examples in order of increasing feature value;\n",
    "    # consider splitting before each index where the value changes\n",
    "    # rather that computing the Gini index from scratch for every split, we will keep track\n",
    "    # of the running frequencies (as we traverse the list of examples ordered by feature value)\n",
    "    # and simply recompute Gini from the frequencies each step (much faster)\n",
    "    \n",
    "    gini=dict()\n",
    "    splits=dict()\n",
    "    for i in range(N-1):\n",
    "        # update frequencies\n",
    "        freq_l[y[i]] += 1/N\n",
    "        freq_u[y[i]] -= 1/N\n",
    "        \n",
    "        if x[i+1] > x[i]: # split here, between i and i+1?\n",
    "            tau = (x[i+1] + x[i]) / 2.0\n",
    "            \n",
    "            gini_l = np.dot(freq_l, 1-freq_l)\n",
    "            gini_u = np.dot(freq_u, 1-freq_u)\n",
    "            \n",
    "            gini[tau] = (i * gini_l + (N-i) * gini_u, gini_l, gini_u)\n",
    "            splits[tau] = i+1 # index of the first element that goes to U\n",
    "\n",
    "    # now find the best split\n",
    "    best_tau = -1\n",
    "    bestval = np.inf\n",
    "    for t in gini:\n",
    "        if gini[t][0] < bestval:\n",
    "            best_tau = t\n",
    "            bestval = gini[t][0]\n",
    "    \n",
    "    L = order[:splits[best_tau]]\n",
    "    U = order[splits[best_tau]:]\n",
    "    \n",
    "    return best_tau, gini[best_tau][1:], L, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 4, 1, 6, 2, 0, 6, 6, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "aa = random.choices(list(range(10)), k=10)\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, min_split_size=5, M=None):\n",
    "    # Find the best way to split the data in X (with labels y) by a single feature\n",
    "    # Don't split if not enough data given min_split_size (return None), otherwise\n",
    "    # return a dictionary with the following key/value pairs:\n",
    "    # 'd' -> the index of the feature in X by which we are splitting;\n",
    "    # 'tau' -> the threshold at which we are splitting that feature;\n",
    "    # 'Gini' -> the (weighted) Gini index of the two new leaves; \n",
    "        # is this the combined Gini index of this node: len(L)*G[0]+len(U)*G[1] ?\n",
    "    # 'Gini-parts' -> a 2-tuple (list) of Gini indices of the L and U leaves after the split;\n",
    "    # 'L' -> indices of the examples in X that go into the lower (left) leaf;\n",
    "    # 'U' -> same for the upper (right) leaf\n",
    "    #\n",
    "    # M specifies the number of features to consider in looking for the best split. If M=None, this means using\n",
    "    # all the features of X. Otherwise, if say M=50, this means pick 50 features at random and only consider \n",
    "    # splitting by these features.\n",
    "    \n",
    "    if X.shape[0] < min_split_size:\n",
    "        return None\n",
    "    num_feat = len(X[0])\n",
    "    feat_indicies = list(range(num_feat))\n",
    "    if M != None:\n",
    "        feat_indicies = random.choices(list(range(num_feat)), k=M)\n",
    "    \n",
    "    best_gini = np.inf\n",
    "    best_d = None\n",
    "    best_tau = None\n",
    "    best_gini_tup = None\n",
    "    best_L = None\n",
    "    best_U = None\n",
    "    \n",
    "    for j in feat_indicies:\n",
    "        x = X[:, j]\n",
    "        tau, G, L, U = best_split_gini(x, y)\n",
    "        if tau == None:\n",
    "            continue\n",
    "\n",
    "        cur_gini = (G[0] * len(L) + G[1] * len(U)) / (len(L) + len(U))\n",
    "        if cur_gini < best_gini:\n",
    "            best_gini = cur_gini\n",
    "            best_d = j\n",
    "            best_tau = tau\n",
    "            best_L = L\n",
    "            best_U = U\n",
    "            best_gini_tup = G\n",
    "    \n",
    "    if best_tau == None:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'd': best_d,\n",
    "        'tau': best_tau, \n",
    "        'Gini': best_gini,\n",
    "        'Gini-parts': best_gini_tup,\n",
    "        'L': best_L, 'U': best_U\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_tree(\n",
    "    root, \n",
    "    data, \n",
    "    labels, \n",
    "    depth, \n",
    "    max_depth=np.inf, \n",
    "    min_split_size=1, \n",
    "    M=None):\n",
    "    # this takes root (a node associated with data/labels, and existing at the given depth)\n",
    "    # and builds a tree rooted at that node, not to exceed the given max_depth\n",
    "   \n",
    "    # take a look at Node.classify() to get a better sense how the recursive nature of the tree\n",
    "    # lets you \"drop a point down the tree\" and assign a label once you are in the leaf\n",
    "\n",
    "    if (depth == max_depth):\n",
    "        no_split=True # we are done, no more splitting below this node\n",
    "    else:\n",
    "        best_split = split_data(data, labels, min_split_size,M)\n",
    "        if best_split==None: # for some reason the splitting code refused to split\n",
    "            no_split=True\n",
    "        else: # splitting!\n",
    "            no_split=False\n",
    "            # COMPLETE CODE: assign values to .index, .val, .is_leaf\n",
    "            root.val = best_split['tau']\n",
    "            root.index = best_split['d']\n",
    "            root.is_leaf = False\n",
    "            \n",
    "            root.left = grow_tree(Node(), data[best_split['L']], labels[best_split['L']], depth+1, max_depth,min_split_size,M)\n",
    "            root.right = grow_tree(Node(), data[best_split['U']], labels[best_split['U']], depth+1, max_depth,min_split_size,M)\n",
    "            \n",
    "            \n",
    "    if no_split: # this is going to be a leaf, let's record this fact and assign it a label to predict\n",
    "        root.is_leaf = True\n",
    "        root.label = label_leaf(labels)\n",
    "         \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, labels, max_depth=np.inf, min_split_size=1, M=None):\n",
    "    root=Node()\n",
    "    tree=grow_tree(root,\n",
    "                   data,\n",
    "                   labels,\n",
    "                   depth=0, # this will \"signal\" to the tree growing function that this is the root\n",
    "                   max_depth=max_depth,\n",
    "                   min_split_size=min_split_size,\n",
    "                   M=M)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tree = build_tree(X, y, max_depth=10, min_split_size=3, M=None) # may take a few minutes\n",
    "tree = build_tree(X, y, max_depth=2, min_split_size=3, M=None) # may take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation accuracy for the tree (ToDo)\n",
    "\n",
    "- **Compute** training and validation accuracies\n",
    "\n",
    "- **Report** training and validation set accuracies of a single tree, \n",
    "    - with **hyperparameters** (max tree depth and minimum size of a leaf) tuned as you see fit (make sure to describe how you tuned them and include the code and output from the process in the notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc is 0.6400666666666667\n",
      "acc is 0.6400666666666667\n",
      "acc is 0.847\n",
      "acc is 0.847\n",
      "acc is 0.8432666666666667\n",
      "acc is 0.8442666666666667\n"
     ]
    }
   ],
   "source": [
    "def test_val_acc(max_depth, min_split_size):\n",
    "    tree = build_tree(X, y, max_depth=max_depth, min_split_size=min_split_size, M=None)\n",
    "    res = np.array(tree.classify_set(X_val))\n",
    "    num_correct = np.sum(res == y_val)\n",
    "    return num_correct / len(y_val)\n",
    "\n",
    "for max_depth in [5, 10, 20]:\n",
    "    for min_split_size in [3, 5]:\n",
    "        print(\"acc is\", test_val_acc(max_depth, min_split_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy (ToDo)\n",
    "\n",
    "- **Compute and report** the results on test set, for the model built with the chosen values – please don’t be tempted to evaluate the model on test more than once, so be sure you are done with the tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the Tree (ToDo)\n",
    "\n",
    "- **Question 4 is a bonus question**. You are encouraged to do it\n",
    "\n",
    "- Add the code for pruning a tree\n",
    "- Report the accuracy of the CART tree (with pruning) using hyperparameters (including λ) tuned as you see fit, on the test set; \n",
    "- Compare it to the result with the unpruned tree, and comment on the difference, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for running bagging ensembles on data\n",
    "def apply_ensemble(trees, data, C=10):\n",
    "    \"\"\"Returns the labels after applying the ensemble\"\"\"\n",
    "    res = np.array([tree.classify_set(data) for tree in trees])\n",
    "    yhat = np.zeros(res.shape[1])\n",
    "    for i in range(res.shape[1]):\n",
    "        yhat[i]=np.argmax(np.bincount(res[:,i]))\n",
    "    return yhat.astype(int)\n",
    "\n",
    "def evaluate_ensemble(trees, data, labels):\n",
    "    \"\"\"Returns the accuracy of the ensemble\"\"\"\n",
    "    return np.mean(apply_ensemble(trees, data) == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ensemble (ToDo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "def create_ensemble(X, y, max_depth=10, min_split_size=1, num_trees=5, M_fraction=0.5):\n",
    "    \"\"\"\n",
    "    M_fraction gives M as a fraction of d (# of features in X)\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    assert 0 < M_fraction < 1.0\n",
    "    M = int(d*M_fraction)  # compute the actual M (number of features to consider in each node)\n",
    "    \n",
    "    trees = []\n",
    "    for _ in tqdm(range(num_trees)): # this will show progress bar as you build the forest\n",
    "        \n",
    "        # COMPLETE CODE HERE -- construct tree\n",
    "        indices = np.random.randint(N, size=N)\n",
    "        tree = build_tree(X[indices], y[indices], max_depth, min_split_size, M)\n",
    "        trees.append(tree)\n",
    "        \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB: This step takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [6:58:39<00:00, 837.32s/it]   \n"
     ]
    }
   ],
   "source": [
    "ensemble = create_ensemble(X, y, max_depth=15, min_split_size=5, num_trees=30, M_fraction=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Accuracy (ToDo)\n",
    "\n",
    "- **Compute** the training and validation accuracy\n",
    "\n",
    "- **Report** the training and validation set accuracies of your ensemble. You should aim to get at least **90%** validation accuracy.\n",
    "- **Plot** the val accuracy as a function of ensemble size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc on training set 0.9971333333333333\n",
      "acc on val set 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [05:51<00:00, 11.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m 0.01 acc 0.9362666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [03:59<35:51, 79.69s/it]"
     ]
    }
   ],
   "source": [
    "# Training acc [todo]\n",
    "print(\"acc on training set\", evaluate_ensemble(ensemble, X, y))\n",
    "print(\"acc on val set\", evaluate_ensemble(ensemble, X_val, y_val))\n",
    "\n",
    "# Tuning for M\n",
    "for m in [0.01, 0.1, 0.15, 0.2]:\n",
    "    ensemble = create_ensemble(X, y, max_depth=15, min_split_size=5, num_trees=30, M_fraction=m)\n",
    "    print(\"m\", m, \"acc\", evaluate_ensemble(ensemble, X_val, y_val))\n",
    "    # best m is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy (ToDo)\n",
    "- **Compute and report** the results of your ensemble on the test set. Again, don’t do it more than once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [37:08<00:00, 74.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc on test set 0.955\n"
     ]
    }
   ],
   "source": [
    "# Test acc with best m [todo]\n",
    "ensemble = create_ensemble(X, y, max_depth=15, min_split_size=5, num_trees=30, M_fraction=0.1)\n",
    "print(\"acc on test set\", evaluate_ensemble(ensemble, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of the size of your ensemble (ToDo)\n",
    "\n",
    "- **Show the effect of the size of your ensemble as follows**: \n",
    "    - Let T be the number of trees in your ensemble. \n",
    "    - For t = 1, . . . , T \n",
    "        - **Compute** the validation set accuracy of the partial ensemble consisting of trees 1, . . . , t\n",
    "        - **Plot** this as a function of t. \n",
    "- Discuss what this tells you and how it may inform tuning T.\n",
    "\n",
    "*Advice: Do not re-run the creation of the forest for T = 1, 2, . . .! rather, take your existing ensemble and compute the accuracy of the partial ensembles consisting of only the first tree; the first two trees; etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] [<__main__.Node object at 0x1181fdee0>, <__main__.Node object at 0x11b691af0>]\n"
     ]
    }
   ],
   "source": [
    "# Test different sizes of ensemble\n",
    "sizes = list(range(len(ensemble)))\n",
    "acc = []\n",
    "trees = []\n",
    "print(\"sizes\", sizes, ensemble[:2])\n",
    "for t in sizes:\n",
    "    partial_ensemble = ensemble[:t]\n",
    "    trees.append(ensemble[t])\n",
    "    val_acc = evaluate_ensemble(trees, X_val, y_val)\n",
    "    acc.append(val_acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size vs acc\n",
    "plt.scatter(sizes, acc)\n",
    "plt.title(\"Effect of the Size of Ensemble\")\n",
    "plt.xlabel(\"Size of Ensemble\")\n",
    "plt.ylabel(\"Val Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss how the plot inform tuning the size of ensemble (ToDo)\n",
    "- Overall, the bigger the size of the ensemble, the better the validation accurary.\n",
    "- At first, val accuracy increases significantly with every tree added to the ensemble. Because each tree has high variance, so averaging a high variance tree with a new tree will ameliorate over-fitting and produce a great improvement in val accuracy. \n",
    "- Yet, when the ensemble size reaches around 15, the increase in accuracy starts to plateau, because the current ensemble already achieved a moderate variance. \n",
    "- Therefore, a good choice of size might be around 15, so that the ensemble creation time would be reduced. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f594f625523eedf37de17555ec746ece4289f4fa39150ea03385bfab3da2cc11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
